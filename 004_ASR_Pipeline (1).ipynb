{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4.0 ASR Pipeline with NVIDIA NeMo\n",
    "## (part of Lab 1)\n",
    "\n",
    "In this notebook, you'll work with the NeMo library to a run the speech recognition pipeline and zoom into each of the steps, including preprocessing modules (spectrograms), acoustic models (AMs), predictions, and post processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.1 Speech Representation](#4.1-Speech-Representation)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1.1 Speech in the Temporal Domain](#4.1.1-Speech-in-the-Temporal-Domain)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1.2 Speech in the Frequency Domain](#4.1.2-Spectrograms)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1.3 Mel Spectrograms](#4.1.3-Mel-Spectrograms)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1.4 Exercise: Reduce the Mel Filter Bands](#4.1.4-Exercise:-Reduce-the-Mel-Filter-Bands)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1.5 Go Further With Speech Representation (Cepstrum and MFC)](#4.1.5-Go-Further-With-Speech-Representation-(Cepstrum-and-MFC))<br>\n",
    "**[4.2 Acoustic Model Architectures](#4.2-Acoustic-Model-Architectures)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.2.1 QuartzNet](#4.2.1-QuartzNet)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.2.2 Citrinet](#4.2.2-Citrinet)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.2.3 Conformer-CTC](#4.2.3-Conformer-CTC)<br>\n",
    "**[4.3 Acoustic Models with NeMo](#4.3-Acoustic-Models-with-NeMo)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3.1 Load QuartzNet](#4.3.1-Load-QuartzNet)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3.2 Load Citrinet](#4.3.2-Load-Citrinet)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3.3 Load Conformer-CTC](#4.3.3-Load-Conformer-CTC)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3.4 Greedy Inference](#4.3.4-Greedy-Inference)<br>\n",
    "**[4.4 Transcript Decoders](#4.4-Transcript-Decoders)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.1 Beam Search Decoder](#4.4.1-Beam-Search-Decoder)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.2 Beam Search Decoder with a Language Model](#4.4.2-Beam-Search-Decoder-with-a-Language-Model)<br>\n",
    "**[4.5 Punctuation and Capitalization](#4.5-Punctuation-and-Capitalization)<br>**\n",
    "**[4.6 Inverse Text Normalization (ITN)](#4.6-Inverse-Text-Normalization-(ITN))<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.6.1 ITN on Other Languages](#4.6.1-ITN-on-Other-Languages)<br>\n",
    "**[4.7 Language Identification](#4.7-Language-Identification)<br>**\n",
    "**[4.8 (Optional) Create Your Own Audio Samples](#4.8-(Optional)-Create-Your-Own-Audio-Samples)<br>**\n",
    "**[4.9 Shut Down the Kernel](#4.9-Shut-Down-the-Kernel)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/asr/ASR_pipeline.PNG\">\n",
    "\n",
    "Building an Automatic Speech Recognition (ASR) pipeline is often the first step in building a conversational AI application. An ASR model converts audio speech into readable text. The main metric used to evaluate ASR models is the Word Error Rate (WER).\n",
    "\n",
    "- **Feature Extraction:** This step converts the temporal audio form of speech to the frequency domain and generates a spectrogram or mel spectrogram.\n",
    "- **Acoustic Model (AM):** The AM is neural network that outputs probabilities over characters, phonemes, or tokens for each time step. In this lab, we will use state-of-the-art AMs: QuartzNet, Citrinet, and Conformer-CTC.\n",
    "- **Decoder and Language Model (LM):** A decoder converts the probability matrix output from the AM into text. The language model is usually used to rescore the likelihood of its text training corpus. \n",
    "- **Punctuation:** Add capitalization and punctuation marks.   \n",
    "- **Inverse Text Normalization (ITN):** Transform the text into readable format using grammar rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.1 Speech Representation\n",
    "To process audio from with Machine Learning techniques, we need to represent it in numerical format. We will see 2 forms of Speech representation: Temporal and frequency domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Speech in the Temporal Domain\n",
    "\n",
    "\n",
    "The most common representation of speech data uses temporal domain which basically represents amplitude changes (vibrations) through time.\n",
    "\n",
    "<img src=\"images/asr/time_domain.png\">\n",
    "\n",
    "Let's have a look at wave file represented in temporal domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "# Import audio processing library\n",
    "import librosa\n",
    "# We'll use this to listen to audio\n",
    "from IPython.display import Audio, display\n",
    "from plotly import graph_objects as go\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load and listen to an audio sample. We will be using the `librosa.load` function. The sampling rate is the number of sampled amplitude values per second. To preserve the sampling rate of the file, we will use `sr=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUDIO_FILENAME = 'dli_workspace/data/audio_sample.wav'\n",
    "\n",
    "# load audio signal with librosa\n",
    "signal, sample_rate = librosa.load(AUDIO_FILENAME, sr=None)\n",
    "duration=librosa.get_duration(y=signal, sr=sample_rate)\n",
    "print(\"Duration:\", duration)\n",
    "print(\"Native sample rate:\", sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display audio player for the signal\n",
    "display(Audio(data=signal, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of the loaded signal. It is a one-dimensional tensor with the amplitude values measured through time. The tensor size is $duration \\times sample\\_rate$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the signal shape\n",
    "print(\"Signal shape:\", signal.shape)\n",
    "print(\"duration x sample_rate =\", duration*sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the signal of the previous wave file in the temporal domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the temporal display layout\n",
    "temporal_layout={\n",
    "        'height': 300,\n",
    "        'xaxis': {'title': 'Time (s)'},\n",
    "        'yaxis': {'title': 'Amplitude'},\n",
    "        'title': 'Rebuilt Audio Signal',\n",
    "        'margin': dict(l=0, r=0, t=40, b=0, pad=0),\n",
    "    }\n",
    "line={'color': 'green'}\n",
    "\n",
    "\n",
    "# Plot the signal in time domain\n",
    "fig_signal = go.Figure(go.Scatter(x=np.arange(signal.shape[0])/sample_rate, y=signal, line=line,name='Waveform',\n",
    "               hovertemplate='Time: %{x:.2f} s<br>Amplitude: %{y:.2f}<br><extra></extra>'),\n",
    "                layout=temporal_layout)\n",
    "fig_signal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Speech in the Frequency Domain\n",
    "\n",
    "\n",
    "Another common way of representing speech is the frequency domain. This consistis in transforming a temporal domain representation to the set of frequencies in the signal using a [Fourier Transform (FT)](https://en.wikipedia.org/wiki/Fourier_transform). The result of a [Discrete Fourier Transform (DFT)](https://en.wikipedia.org/wiki/Discrete_Fourier_transform) algorithm (a discrete version of FT) is called a spectrogram. A [Short Time Fourier transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) represents a signal in the time-frequency domain calculated with DFTs over short overlapping windows.\n",
    "\n",
    "Let's see the Short-Time-Fourier-Transform in action:\n",
    "- `n_fft`: Short segments length\n",
    "- `time_stride`: segments overlap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the STFT\n",
    "time_stride=0.01\n",
    "hop_length = int(sample_rate*time_stride)\n",
    "n_fft = 512\n",
    "\n",
    "# linear scale spectrogram\n",
    "s_stft = librosa.stft(y=signal, n_fft=n_fft,hop_length=hop_length)\n",
    "\n",
    "print(\"hop_length is {}\".format(hop_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the spectrogram generated by the STFT operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the frequency display layout\n",
    "colorscale=[ [0, 'rgb(30,62,62)'], [0.5, 'rgb(30,128,128)'], [1, 'rgb(30,255,30)'],]\n",
    "colorbar=dict(ticksuffix=' dB')\n",
    "frequency_layout={     'height': 300,\n",
    "        'xaxis': {'title': 'Time (s)'},\n",
    "        'yaxis': {'title': 'Frequency (kHz)'},\n",
    "        'title': 'Spectrogram',\n",
    "        'margin': dict(l=0, r=0, t=40, b=0, pad=0),\n",
    "    }\n",
    "\n",
    "# Convert a power spectrogram (amplitude squared) to decibel (dB) units\n",
    "s_stft_db = librosa.power_to_db(np.abs(s_stft)**2, ref=np.max, top_db=100)\n",
    "\n",
    "# Plot the spectrogram\n",
    "fig_spectrum = go.Figure(go.Heatmap(z=s_stft_db, colorscale=colorscale,colorbar=colorbar, name='Spectrogram',\n",
    "               hovertemplate='Time: %{x:.2f} s<br>Frequency: %{y:.2f} kHz<br>Magnitude: %{z:.2f} dB<extra></extra>'),layout=frequency_layout)\n",
    "fig_spectrum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the frequency domain, it is possible to roll back to the temporal domain using inverse STFT. \n",
    "Let's generate the temporal domain representation of the audio from the spectrogram and listen to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse STFT\n",
    "signal_hat = librosa.istft(s_stft, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# Plot the converted signal in time domain\n",
    "fig_signal = go.Figure( go.Scatter(x=np.arange(signal_hat.shape[0])/sample_rate, y=signal_hat, line=line,\n",
    "               name='Waveform',hovertemplate='Time: %{x:.2f} s<br>Amplitude: %{y:.2f}<br><extra></extra>'),layout=temporal_layout)\n",
    "fig_signal.show()\n",
    "\n",
    "# Listen to the converted signal  \n",
    "display(Audio(data=signal_hat, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1.3 Mel Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human ear perceives the differences between frequencies in a non-linear way. Indeed, the difference in low frequencies are more perceptible than difference in high frequencies. The mel scale representation allows us to reproduce this effect by making the signal more discriminative for low frequencies and less discriminative for high frequencies. In speech processing, mel filter banks can be applied to convert spectrograms to match the human perception of the distances between frequencies.\n",
    "\n",
    "Let's create a mel filter bank with 16 mel bands (filters) using the function `librosa.filters.mel` and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mel spectrogram display layout\n",
    "\n",
    "mel_colorbar=dict(ticksuffix=' dB')\n",
    "mel_colorscale=[\n",
    "                   [0, 'rgb(30,62,62)'],\n",
    "                   [0.5, 'rgb(30,128,128)'],\n",
    "                   [1, 'rgb(30,255,30)'],\n",
    "               ]\n",
    "mel_layout={\n",
    "        'height': 500,\n",
    "        'xaxis': {'title': 'Frequency (kHz)'},\n",
    "        'yaxis': {'title': 'Mel Filters'},\n",
    "        'title': 'Mel Filter Bank',\n",
    "        'margin': dict(l=0, r=0, t=40, b=0, pad=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of mel bands to generate\n",
    "n_mels = 16\n",
    "\n",
    "# Create a mel filter bank.\n",
    "mel_16 = librosa.filters.mel(sr=sample_rate, n_fft=n_fft, n_mels=n_mels)\n",
    "\n",
    "# Plot the mel filter bank\n",
    "fig_spectrum = go.Figure(go.Heatmap(z=mel_16,colorscale =mel_colorscale, colorbar=mel_colorbar, ygap=0.1, name='Spectrogram',\n",
    "               hovertemplate='Time: %{x:.2f} s<br>Frequency: %{y:.2f} kHz<br>Magnitude: %{z:.2f} dB<extra></extra>'), layout=mel_layout)\n",
    "fig_spectrum.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the spectrogram to the mel scale with 128 filters. We will use the function `librosa.feature.melspectrogram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mel spectrogram\n",
    "n_mels = 128\n",
    "n_fft = 512\n",
    "\n",
    "# Mel scale spectrogram\n",
    "S = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "# Convert a power spectrogram (amplitude squared) to decibel (dB) units\n",
    "melspectrogram_DB = librosa.power_to_db(np.abs(S)**2, ref=np.max, top_db=100)\n",
    "\n",
    "# plot the signal in frequency domain\n",
    "fig_spectrum = go.Figure(go.Heatmap(z=melspectrogram_DB, colorscale=colorscale,colorbar=dict(ticksuffix=' dB'),name='Mel-Spectrogram',\n",
    "               hovertemplate='Time: %{x:.2f} s<br>Frequency: %{y:.2f} kHz<br>Magnitude: %{z:.2f} dB<extra></extra>'),layout=frequency_layout)\n",
    "fig_spectrum.show()\n",
    "\n",
    "# Plot the spectrogram\n",
    "fig_spectrum = go.Figure(go.Heatmap(z=s_stft_db, colorscale=colorscale,colorbar=colorbar, name='Spectrogram',\n",
    "               hovertemplate='Time: %{x:.2f} s<br>Frequency: %{y:.2f} kHz<br>Magnitude: %{z:.2f} dB<extra></extra>'),layout=frequency_layout)\n",
    "fig_spectrum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between low frequencies is stretched.\n",
    "\n",
    "## 4.1.4 Exercise: Reduce the Mel Filter Bands\n",
    "Try to apply a mel scale to the previous spectrogram with only 16 mel bands. Convert to decibels and visualize the results. If you get stuck, refer to the [solution](solutions/ex4.1.4.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel spectrogram\n",
    "\n",
    "n_fft = #FIXME\n",
    "n_mels = #FIXME\n",
    "\n",
    "# Mel scale spectrogram\n",
    "melspectrogram = #FIXME\n",
    "\n",
    "# Convert a power mel spectrogram (amplitude squared) to decibel (dB) units\n",
    "melspectrogram_DB = #FIXME\n",
    "\n",
    "# plot the signal in the frequency domain\n",
    "fig_spectrum = go.Figure(go.Heatmap(z=melspectrogram_DB, colorscale=colorscale,colorbar=dict(ticksuffix=' dB'),name='Mel-Spectrogram',\n",
    "               hovertemplate='Time: %{x:.2f} s<br>Frequency: %{y:.2f} kHz<br>Magnitude: %{z:.2f} dB<extra></extra>'),layout=frequency_layout)\n",
    "fig_spectrum.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.5 Go Further With Speech Representation (Cepstrum and MFC)\n",
    "By applying a second FT on the spectrograms, we get the _cepstrum_. This represents the variations between the frequency bands.\n",
    "\n",
    "Features derived from the cepstrum describe speech better than features taken directly from the frequency spectrum.\n",
    "\n",
    "_Mel frequency cepstrum (MFC)_ uses a [cosine transform (CT)](https://en.wikipedia.org/wiki/Discrete_cosine_transform) instead of FT applied on the mel spectrogram. The CT is well suited for compression as it extracts only the real part of the signal. So MFCs are cepstrums equal to spectrum-of-a-log-spectrums of signals. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zodyzdyTVXas"
   },
   "source": [
    "---\n",
    "# 4.2 Acoustic Model Architectures\n",
    "\n",
    "In this lab, we will experiment with three acoustic models: [QuartzNet](https://arxiv.org/pdf/1910.10261.pdf) and [Citrinet](https://arxiv.org/pdf/2104.01721.pdf), and Conformer-CTC.  Let's begin by exploring the architectures of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 QuartzNet\n",
    "_QuartzNet_ is a deep neural model for speech recognition developed by NVIDIA Research. The network is divided into:\n",
    "- Encoder - trains the acoustic features representation\n",
    "- Decoder - maps those features to the vocabulary (characters or phonemes).  \n",
    "\n",
    "QuartzNet is a variant of the _NVIDIA Jasper_ model [(Just Another Speech Recognizer)](https://arxiv.org/pdf/1904.03288.pdf).  However, QuartzNet replaces Jasper's 1D convolutions with 1D time-channel separable convolutions, which use fewer parameters while keeping a similar accuracy. QuartzNet uses a non-autoregressive CTC-based (Connectionist Temporal Classification) decoding scheme, which means that it does not require manual alignment between the input and output pairs. Learn more about the [CTC loss](https://www.cs.toronto.edu/~graves/icml_2006.pdf).\n",
    "\n",
    "<img src=\"images/asr/quartz_vertical.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Citrinet\n",
    "\n",
    "_Citrinet_ is a variant of QuartzNet, developed by NVIDIA Research. Unlike QuartzNet, which predicts characters or phonemes, Citrinet uses subword encoding via WordPiece tokenization. This results in performance improvement of the audio transcripts.\n",
    "\n",
    "<img src=\"images/asr/citrinet_vertical.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 Conformer-CTC\n",
    "\n",
    "The _Conformer_ model uses the combination of self-attention and convolution modules to achieve the best of the two approaches.  The self-attention layers can learn the global interaction while the convolutions efficiently capture the local correlations. The self-attention modules support both regular self-attention with absolute positional encoding, and also Transformer-XL’s self-attention with relative positional encodings. \n",
    "\n",
    "_Conformer-CTC_ has a similar encoder to the original Conformer but uses CTC loss and decoding instead of RNNT/Transducer loss, which makes it a non-autoregressive model. We also drop the LSTM decoder and, instead, use a linear decoder on the top of the encoder. \n",
    "\n",
    "Here is the overall architecture of the encoder of Conformer-CTC:\n",
    "\n",
    "<img src=\"images/asr/conformer-encoder.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.3 Acoustic Models with NeMo\n",
    "\n",
    "In the NeMo library, ASR models are defined under the `nemo_asr.models.ASRModel` method. \n",
    "\n",
    "To load an acoustic model, it is possible to restore the parameters from a local `.nemo` model. \n",
    "\n",
    "Alternatively, pretrained models can be loaded from the NVIDIA Repository, NGC, using the `from_pretrained(...)` method that downloads and initializes models directly from the cloud. To check the list of available pretrained models, please use the `list_available_models` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "def display_list_available_models(model):\n",
    "    print ( \"list of available models:\")\n",
    "    for m in model.list_available_models():\n",
    "        print (\"   \", \"\\033[1;34m\", m.pretrained_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_list_available_models(nemo_asr.models.ASRModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2LMVI9qqtEV"
   },
   "source": [
    "## 4.3.1 Load QuartzNet\n",
    "\n",
    "Let's load a base English QuartzNet15x5 model, `stt_en_quartznet15x5`. The speech-to-text English QuartzNet model was trained on a combination of seven datasets of English speech, with a total of 7,057 hours of audio samples. Samples were limited to a minimum duration of 0.1s long, and a maximum duration of 16.7s long. The model was trained for 300 epochs with [Automatic Mixed Precision (AMP)](https://developer.nvidia.com/automatic-mixed-precision).\n",
    "It achieves a Word Error Rate (WER) of 4.38% on the [LibriSpeech](https://www.openslr.org/12) dev-clean dataset, and a WER of 11.30% on the LibriSpeech dev-other dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhWmR7lbvwSm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "am_model_quartznet = nemo_asr.models.ASRModel.from_pretrained(model_name='stt_en_quartznet15x5', strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check its vocabulary. QuartzNet is a character-based ASR model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check vocabulary\n",
    "am_model_quartznet.decoder.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 Load Citrinet\n",
    "\n",
    "The English Citrinet-512 model has 36 million parameters. This model was trained on the ASR dataset with over 7000 hours of English speech. It uses the [SentencePiece](https://github.com/google/sentencepiece) tokenizer with a vocabulary size at 1024, and transcribes text into lower case English along with spaces, apostrophes, and a few other characters.\n",
    "\n",
    "The WER achieved by the English Citrinet-512 model is 3.7% on LibriSpeech (test-clean) and 3.2% on the Wall Street Journal (WSJ) (Eval 92). More details can be found on the [NGC stt_en_citrinet_512 model card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_citrinet_512).\n",
    "\n",
    "Let's load the model, `stt_en_citrinet_512`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "am_model_citrinet = nemo_asr.models.ASRModel.from_pretrained(model_name=\"stt_en_citrinet_512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at Citrinet's vocabulary. Citrinet is a subword-based ASR model. This particular model can decode speech into 1024 subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "am_model_citrinet.decoder.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 Load Conformer-CTC\n",
    "\n",
    "The base English Conformer-CTC Large model has around 120M parameters. This model was trained on about 24500 hours of English speech. It uses the [SentencePiece](https://github.com/google/sentencepiece) tokenizer with a vocabulary size of 128, and transcribes text in lower case English along with spaces, apostrophes, and a few other characters.\n",
    "The WER achieved by English Conformer-CTC large model is 2.1% on LibriSpeech (test-clean) and 1.7% on the WSJ (Eval 92). More details can be found on the [NGC stt_en_conformer_ctc_large model card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_conformer_ctc_large).\n",
    "\n",
    "Let's load the model `stt_en_conformer_ctc_large`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes few seconds\n",
    "am_model_conformer = nemo_asr.models.ASRModel.from_pretrained(model_name=\"stt_en_conformer_ctc_large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Conformer-CTC vocabulary.  Similar to Citrinet, Conformer-CTC model is a subword-based ASR model. This particular Conformer-CTC vocabulary size is 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "am_model_conformer.decoder.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQSj-IhEhrtI"
   },
   "source": [
    "## 4.3.4 ASR Inference with Greedy Decoder\n",
    "\n",
    "If we have an entire audio clip available, we can perform offline inference with the models previously loaded (QuartzNet, Citrinet, and Conformer) to transcribe it in greedy mode. A _greedy decoder_ simply chooses the token with the highest probability at each time step to build the transcriptions. \n",
    " \n",
    "The easiest way to do this is to call the `transcribe(...)` method, which transcribes multiple files in a batch, applying a CTC greedy decoder to raw probability distributions over the alphabet characters from the ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = [AUDIO_FILENAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0ERrXIzKpwu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QuartzNet transcription \n",
    "transcript = am_model_quartznet.transcribe(paths2audio_files=files)[0]\n",
    "print(f'QuartzNet Transcript: \"{transcript}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Citrinet transcription\n",
    "transcript = am_model_citrinet.transcribe(paths2audio_files=files)[0]\n",
    "print(f'Citrinet Transcript: \"{transcript}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformer-CTC transcription\n",
    "files = [AUDIO_FILENAME]\n",
    "transcript = am_model_conformer.transcribe(paths2audio_files=files)[0]\n",
    "print(f'Conformer-CTC Transcript: \"{transcript}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UOoj-WfQoL_"
   },
   "source": [
    "How do you find the performance of AMs in greedy mode for the:\n",
    "- QuartzNet model?\n",
    "- Citrinet model?\n",
    "- Conformer-CTC model?\n",
    "Discuss the performance with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Acoustic Model Outputs\n",
    "To get log probabilities output by the Acoustic Model, we can set the argument `logprobs=True` when querying for transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits= am_model_conformer.transcribe(files, logprobs=True)\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the log probabilities output by AM models for each time step. \n",
    "\n",
    "First, run the next cell to define the display setup. And then, run the following cells to visualize the log probabilities per time step for QuartzNet, Citrinet, and Conformer. For the QuartzNet AM, you should see:\n",
    "\n",
    "<img src=\"images/asr/log_prob_quartznet.PNG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0Sk0C9-LmAR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_layout={\n",
    "            'height': 2000,\n",
    "            'xaxis': {'title': 'Time, s'},\n",
    "            'yaxis': {'title': 'Vocabulary'},\n",
    "            'title': 'Vocabulary Probabilities',\n",
    "            'margin': dict(l=0, r=0, t=40, b=0, pad=0),\n",
    "        }\n",
    "prob_colorscale=[[0, 'rgb(220,220,220)'], [1, 'rgb(255,0,0)'],]\n",
    "\n",
    "# softmax implementation in NumPy\n",
    "def softmax(logits):\n",
    "    e = np.exp(logits - np.max(logits))\n",
    "    return e / e.sum(axis=-1).reshape([logits.shape[0], 1])\n",
    "\n",
    "def display_logprobs(asr_model, files):\n",
    "    # let's do inference once again but without decoder\n",
    "    logits = asr_model.transcribe(files, logprobs=True)[0]\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    # 20ms is duration of a timestep at output of the model\n",
    "    time_stride = 0.01\n",
    "\n",
    "    # get model's alphabet\n",
    "    labels = list(asr_model.decoder.vocabulary) + ['blank']\n",
    "    labels[0] = 'space'\n",
    "\n",
    "    # plot probability distribution over characters for each timestep\n",
    "    fig_probs = go.Figure(\n",
    "        go.Heatmap(z=probs.transpose(),colorscale=prob_colorscale, y=labels, dx=time_stride, name='Probs', ygap=1,xgap=0.1,\n",
    "                   hovertemplate='Time: %{x:.2f} s<br>Character: %{y}<br>Probability: %{z:.2f}<extra></extra>' ),layout = prob_layout)\n",
    "    fig_probs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the log probabilities per time step for QuartzNet\n",
    "display_logprobs(am_model_quartznet, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the log probabilities per time step for Citrinet\n",
    "display_logprobs(am_model_citrinet, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the log probabilities per time step for Conformer\n",
    "display_logprobs(am_model_conformer, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiNMZBodIaSP"
   },
   "source": [
    "It is easy to identify time steps for the `blank` character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8Jvwe4Ahncx"
   },
   "source": [
    "---\n",
    "# 4.4 Transcript Decoders\n",
    "\n",
    "Acoustic models output log probabilities across their vocabularies at each time step. Decoders are applied on top of this matrix to find the best transcription candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 Beam Search Decoder\n",
    "\n",
    "While the greedy decoder selects the highest probability at each time step, the _beam search decoder_ keeps track of several best sequences at each step. The _beam size_ limits the number of candidates to consider by fixed number of beams to keep and expand upon.  The selection of beams is based on the highest probabilities of the sequence.\n",
    "\n",
    "The probability of a sequence composed on three words $W_1 W_2 W_3$ is computed as follows: $P(W_1 W_2 W_3) = p(W_1) * p(W_2/W_1) * p(W_3/W_1 W_2) $\n",
    "\n",
    "With NeMo, we can set up the beam search decoder using the `BeamSearchDecoderWithLM` module.\n",
    " \n",
    "Run the next three cells to: \n",
    "- Set up a beam search decoder with beam size of 16\n",
    "- Recompute the log probabilities of the previous audio sample using QuartzNet\n",
    "- Run the beam search decoder using the `BeamSearchDecoderWithLM.forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of CPUs to use. Set to the max when processing large batches of log probabilities\n",
    "num_cpus=max(os.cpu_count(), 1)\n",
    "\n",
    "# Set the beam size\n",
    "beam_size=16\n",
    "\n",
    "# Get the vocabulary size \n",
    "vocab=list(am_model_quartznet.decoder.vocabulary)\n",
    "\n",
    "# Beam search\n",
    "beam_search = nemo_asr.modules.BeamSearchDecoderWithLM(beam_width=beam_size, lm_path=None, alpha=None, beta=None, vocab=vocab, num_cpus=num_cpus, input_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recompute the log probabilities of the previous audio sample using QuartzNet\n",
    "files = [AUDIO_FILENAME]\n",
    "logits = am_model_quartznet.transcribe(files, logprobs=True)[0]\n",
    "probs = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the beam search decoder\n",
    "best_sequences=beam_search.forward(log_probs = np.expand_dims(probs, axis=0), log_probs_length=None)\n",
    "print( \"Number of best sequences :\", len(best_sequences[0]))\n",
    "print( \"Best sequences :\")\n",
    "best_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you find the performance of QuartzNet acoustic model with a beam search decoder? Discuss that with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 Beam Search Decoder with a Language Model\n",
    "\n",
    "A vanilla beam search decoder allows us to catch some spelling errors, but it doesn't take into account the language modeling. A language model (LM) measures the correctness of a sentence by scoring its probability based on the text corpus used to train the LM. \n",
    "\n",
    "A LM can be used with the beam search decoders to produce more accurate candidates. The score calculation formula is:\n",
    "\n",
    "$final\\_score = acoustic\\_score + beam\\_a\\mathbf{}lpha*lm\\_score + beam\\_beta*seq\\_length$\n",
    "\n",
    "where:\n",
    "- `acoustic_score` is the score predicted by the acoustic encoder\n",
    "- `lm_score` is the score predicted by the language model\n",
    "- `beam_alpha` specifies the importance of the language model\n",
    "- `beam_beta` is a penalty term according to the sequence length\n",
    "\n",
    "\n",
    "It is possible to use an external [KenLM](https://kheafield.com/code/kenlm/)-based N-GRAM language model to rescore multiple transcription candidates. \n",
    "\n",
    "In the next, we will:\n",
    "- Download an n-gram language model from NGC. This is a simple 4-gram language model trained with Kneser-Ney smoothing using KenLM library\n",
    "- Load the n-gram language model with the `kenlm` library\n",
    "- Score several options for nvidia spelling\n",
    "- Run the beam search decoder with the n-gram LM on the QuartzNet acoustic model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from NGC \n",
    "!ngc registry model download-version \"nvidia/tao/speechtotext_en_gb_lm:deployable_v1.0\" \\\n",
    "    --dest dli_workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the downloaded n-gram language models\n",
    "! ls dli_workspace/speechtotext_en_gb_lm_vdeployable_v1.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install kenlm package\n",
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "# Load n-gram Language Model\n",
    "EN_GB_LM='/dli_workspace/speechtotext_en_gb_lm_vdeployable_v1.0/en_gb_comp_norm_2.1_3gram.bin'\n",
    "model = kenlm.Model(EN_GB_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Score several options for nvidia\n",
    "print(\"Language Model score for 'in vidia' is:\", model.score('in vidia'))\n",
    "print(\"Language Model score for 'an vidia' is:\", model.score('an vidia'))\n",
    "print(\"Language Model score for 'invidia' is:\", model.score('invidia'))\n",
    "print(\"Language Model score for 'anvidia' is:\", model.score('anvidia'))\n",
    "print(\"Language Model score for 'nvidia' is:\", model.score('nvidia'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLDbUkzzUAqW"
   },
   "source": [
    "Let's now instantiate the Beam Search decoder with a Language Model using `BeamSearchDecoderWithLM` module by provising $lm_path$, $alpha = 2$ and $beta = 1.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qgKa9L954bJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Beam search with LM rescoring\n",
    "beam_search_lm= nemo_asr.modules.BeamSearchDecoderWithLM(beam_width=beam_size, lm_path=EN_GB_LM, alpha=2, beta=1.5, vocab=vocab, num_cpus=num_cpus, input_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply beam search with LM\n",
    "hypothesis=beam_search_lm.forward(log_probs = np.expand_dims(probs, axis=0), log_probs_length=None)\n",
    "\n",
    "# Get the best hypothesis\n",
    "best_hypothesis=hypothesis[0][0]\n",
    "print(\"Best hypothesis using the Beam Search decoder with a Language Model: \", best_hypothesis[1])\n",
    "print(\"Best hypothesis score :\", best_hypothesis[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you find the performance when applying a beam search decoder with the n-gram LM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.5 Punctuation and Capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our Automatic Speech Recognition (ASR) pipeline generated text with no punctuation and capitalization of the words. \n",
    "In conversational AI applications, the ASR output could be used as input to Natural Language Understanding (NLU) modules. Punctuation and capitalization add more information for the NLU modules and could potentially boost their performance.\n",
    "\n",
    "We will be using a BERT base uncased language model fine-tuned with two token classification heads for:\n",
    "- Predicting a punctuation mark that should follow the word (if any). The model supports commas, periods, and question marks.\n",
    "- predicting if the word should be capitalized or not.\n",
    "\n",
    "Let's check available NeMo `PunctuationCapitalizationModel` models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load relevant libraries\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_list_available_models(nemo_nlp.models.PunctuationCapitalizationModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the [punctuation_en_bert](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/punctuationcapitalization_english_bert) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load a punctuation and capitalization NeMo model\n",
    "model = nemo_nlp.models.PunctuationCapitalizationModel.from_pretrained(model_name=\"punctuation_en_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the punctuation and capitalization model using the `add_punctuation_capitalization` function that can take in a list of non-capitalized and punctuated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run punctuation and capitalization on some text\n",
    "list_text=['really','how are you doing', 'great how about you']\n",
    "\n",
    "list_text_PC=model.add_punctuation_capitalization(list_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show punctuation and capitalization results\n",
    "print(\"Capitalization and Punctuation Results:\")\n",
    "results = \"\\n\".join(\"      {} --> {}\".format(x, y) for x, y in zip(list_text, list_text_PC))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go back to our transcriptions problem and run the punctuation and capitalization on the best transcription hypothesis resulted from the Beam Search Decoder with a LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run punctuation and capitalization of our best transcription hypothesis\n",
    "best_hypothesis_cp=model.add_punctuation_capitalization([best_hypothesis[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best transcription with capitalization and punctuation: \", best_hypothesis_cp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you find the performance of the ASR pipeline built so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Inverse Text Normalization (ITN)\n",
    "\n",
    "_Inverse text normalization (ITN)_ converts spoken transcriptions into their written format. For example, the ITN module converts the spoken transcription, \"one hundred and twenty three dollars\" into the written format, \"$123\".\n",
    "\n",
    "State-of-the-art ITN uses grammar rules for several concepts such as dates, measures, time, telephone numbers, electronics, money, and so on. Learn more about NeMo ITN at [NeMo Inverse Text Normalization: From Development To Production 2021.](https://arxiv.org/abs/2104.05055)\n",
    "\n",
    "In the cells below, we load ITN grammar rules for English and inverse normalize some text using the `inverse_normalizer_en.inverse_normalize()` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inverse text normalization instance\n",
    "from nemo_text_processing.inverse_text_normalization.inverse_normalize import InverseNormalizer\n",
    "inverse_normalizer_en = InverseNormalizer(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_sample1  = \"She was born on the sixth of september two thousand and one in London.\"\n",
    "\n",
    "# Run ITN on example string input\n",
    "transcriptions_normalized_sapmle1 = inverse_normalizer_en.inverse_normalize(transcriptions_sample1, verbose=False)\n",
    "\n",
    "# plot the results\n",
    "print(\"Reference transcription:\", transcriptions_sample1)\n",
    "print(\"Normalized transcription:\", transcriptions_normalized_sapmle1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to output the details of the annotated text provided by the ITN model by setting the `verbose=True` argument. \n",
    "Bellow an example output for the sentence \"one hundred and twenty three dollars\":\n",
    "```\n",
    "tokens { money { integer_part: \"123\" currency: \"$\" } }\n",
    "```\n",
    "\n",
    "Let's have a look at the classifications on our previous sample:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_normalized_sapmle1 = inverse_normalizer_en.inverse_normalize(transcriptions_sample1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another example for ITN of phone number and email transcriptions. The English ITN is set for a standard American telephone number with ten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_sample2 = \"My phone number is five five five five five five one two three four and my email is dana at nvidia dot com\"\n",
    "\n",
    "# run ITN on example string input\n",
    "transcriptions_normalized_sapmle2 = inverse_normalizer_en.inverse_normalize(transcriptions_sample2 , verbose=False)\n",
    "\n",
    "# plot the results\n",
    "print(\"Reference transcription:\", transcriptions_sample2)\n",
    "print(\"Normalized transcription:\", transcriptions_normalized_sapmle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.6.1 ITN on Other Languages\n",
    "\n",
    "In addition to the English model, NeMo ITN offers rules for German (de), Spanish (es), French (fr), Portuguese (pt), Russian (ru), and Vietnamese (vi). Let's try to inverse normalize a French sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inverse text normalization instance for french\n",
    "from nemo_text_processing.inverse_text_normalization.inverse_normalize import InverseNormalizer\n",
    "inverse_normalizer_fr = InverseNormalizer(lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = \"Le coup total pour l'année deux mille vingt-deux est de quatre cent millions d'euro\"\n",
    "\n",
    "# run ITN on example string input\n",
    "transcriptions_normalized = inverse_normalizer_fr.inverse_normalize(transcriptions , verbose=False)\n",
    "\n",
    "# plot the results\n",
    "print(\"Reference transcription:\", transcriptions)\n",
    "print(\"Normalized transcription:\", transcriptions_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to customize the ITN grammar rules. Learn more in the dedicated  [NeMo tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/text_processing/WFST_Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 4.7 Language Identification\n",
    "\n",
    "[Speech classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/intro.html) refers to a set of tasks to automatically classify input utterances or audio segments into categories.  Speech classification tasks include _speech command recognition_ (multi-class), _VAD or voice activity detection_ (binary or multi-class), and _audio sentiment classification_ (typically multi-class), etc.  To learn more, see the [list of available models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/results.html#speech-classification-models).\n",
    "\n",
    "_Spoken language identification_ (Lang ID), also known as _spoken language recognition_, is the task of recognizing the language of the spoken utterance automatically. It typically is used in preprocessing ASR, determining which ASR model should be activated based on the language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_list_available_models(nemo_asr.models.EncDecSpeakerLabelModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the language detector model [langid_ambernet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/langid_ambernet). This model is based on AmberNet architecture trained on 6628 hours. The average amount of data per language is 62 hours. Model achieves 5.22% error rate on official evaluation set which contains 1609 verified utterances of 33 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "langid_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained(model_name=\"langid_ambernet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUDIO_SAMPLES = \"/opt/nvidia-riva/tutorials/audio_samples\"\n",
    "!ls $AUDIO_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use get_label() to identify the language (available with NeMo 1.14.0 and later)\n",
    "wav_file = 'en-US_sample.wav'\n",
    "lang = langid_model.get_label(AUDIO_SAMPLES + '/' + wav_file)\n",
    "print(\"The language code for '{}' is: {}\".format(wav_file, lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish\n",
    "wav_file = 'es-US_sample.wav'\n",
    "lang = langid_model.get_label(AUDIO_SAMPLES + '/' + wav_file)\n",
    "print(\"The language code for '{}' is: {}\".format(wav_file, lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian\n",
    "wav_file = 'ru-RU_sample.wav'\n",
    "lang = langid_model.get_label(AUDIO_SAMPLES + '/' + wav_file)\n",
    "print(\"The language code for '{}' is: {}\".format(wav_file, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 4.8 (Optional) Create Your Own Audio Samples\n",
    "\n",
    "You can upload your own audio samples to try the ASR performance. \n",
    "The files should be `.wav` format, resampled to 16kHz. \n",
    "Here is a [torchaudio](https://pytorch.org/audio/stable/index.html)-based example for `.wav` file resampling.\n",
    "\n",
    "```\n",
    "import torchaudio\n",
    "input_wav_file = '/path/to/my_audio.wav'\n",
    "output_wav_file = '/path/to/my_audio_resampled.wav'\n",
    "y, sr = torchaudio.load(input_wav_file)\n",
    "y = y.mean(dim=0) # if there are multiple channels, average them to single channel\n",
    "if sr != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "    y_resampled = resampler(y).unsqueeze(0)\n",
    "    torchaudio.save(output_wav_file, y_resampled, 16000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.9 Shut Down the Kernel\n",
    "<h3 style=\"color:red;\">Important!</h3>\n",
    "\n",
    "From the menu above, choose ***Kernel->Shut Down Kernel*** to fully clear GPU memory before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Gained an understanding about the ASR pipeline and the various acoustic models\n",
    "- Used transcript decoders to select the best transcriptions\n",
    "- Used ITN, capitalization, and punctuation to improve written transcriptions\n",
    "- Used a language identification model to identify what language was spoken\n",
    "\n",
    "Next, you'll deploy the model on Riva. Move on to [Deployment with Riva](005_ASR_Deployment.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Offline_ASR.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
