{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22120735",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726d94b",
   "metadata": {},
   "source": [
    "# 11.0 Deploying Riva Services within a Kubernetes Cluster and Further Riva API Examples \n",
    "## (part of Lab 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c8bfb",
   "metadata": {},
   "source": [
    "In this notebook, you'll deploy NVIDIA Riva within Kubernetes, and try some API queries for text-to-speech (TTS) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b61b9d",
   "metadata": {},
   "source": [
    "**[11.1 Deploy NVIDIA Riva](#11.1-Deploy-NVIDIA-Riva)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.1.1 Exercise: Configure Helm Values and Deploy](#11.1.1-Exercise:-Configure-Helm-Values-and-Deploy)<br>\n",
    "**[11.2 Riva Services](#11.2-Riva-Services)<br>**\n",
    "**[11.3 Riva TTS Example](#11.3-Riva-TTS-Example)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.1 Exercise: Pod IP with Port 50051](#11.3.1-Exercise:-Pod-IP-with-Port-50051)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.2 Exercise: LoadBalancer IP with Port 50051](#11.3.2-Exercise:-LoadBalancer-IP-with-Port-50051)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.3 Exercise: Localhost with Mapped Port](#11.3.3-Exercise:-Localhost-with-Mapped-Port)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.3.4 Upgrade the Service with Helm](#11.3.4-Upgrade-the-Service-with-Helm)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[11.3.4.1 Exercise: Upgrade the Service Type to NodePort](#11.3.4.1-Exercise:-Upgrade-the-Service-Type-to-NodePort)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[11.3.4.2 Verify the Upgrade](#11.3.4.2-Verify-the-Upgrade)<br>\n",
    "**[11.4 Riva NLP Examples](#11.4-Riva-NLP-Examples)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.4.1 `analyze_intent` API](#11.4.1-analyze_intent-API)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.4.2 `punctuate_text` API](#11.4.2-punctuate_text-API)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[11.4.3 Shutdown](#11.4.3-Shutdown)<br>\n",
    "\n",
    "In the previous parts of the class, you have deployed Riva using very basic shell commands. \n",
    "You have also deployed a basic CUDA application to a Kubernetes cluster.\n",
    "Now it is time to put it all together and deploy Riva into production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f32282",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "1. The steps in this notebook assume that you are starting with a K8s cluster that is GPU enabled with feature discovery.  Let's ensure that by stopping and restarting the a cluster and bringing it to a known state. \n",
    "2. As with earlier NVIDIA Riva deployments, you need NGC API credentials.  In this case, you'll also need your email address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af749b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete and restart K8s\n",
    "!minikube delete\n",
    "!minikube start --driver=none\n",
    "# Install the GPU device plugin with Helm\n",
    "!helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\\n",
    "   && helm repo update\n",
    "!helm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n",
    "  --namespace nvidia-device-plugin \\\n",
    "  --create-namespace \\\n",
    "  --version 0.13.0\n",
    "# Install GPU feature discovery with Helm\n",
    "!helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery \\\n",
    "    && helm repo update\n",
    "!helm upgrade -i nvgfd nvgfd/gpu-feature-discovery \\\n",
    "  --version 0.7.0 \\\n",
    "  --namespace gpu-feature-discovery \\\n",
    "  --create-namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your personal API key and email address (valid in the scope of this notebook)\n",
    "NGC_API_KEY = \"YOUR_NGC_API_KEY\"\n",
    "NGC_EMAIL = \"YOUR_EMAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Copy all the ASR and TTS models for convenience (faster deployment)\n",
    "# Time is about 1-2 minutes for the copy unless already done previously \n",
    "cp -rn  /dli_workspace/riva-asr-model-repo/* \\\n",
    "    /dli_workspace/riva-full-model-repo/\n",
    "cp -rn  /dli_workspace/riva-tts-model-repo/* \\\n",
    "    /dli_workspace/riva-full-model-repo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf1319",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.1 Deploy NVIDIA Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb5452",
   "metadata": {},
   "source": [
    "The instructions for deploying NVIDIA Riva on Kubernetes are available on the [NGC Riva Speech Skills Helm chart](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/helm-charts/riva-api) page.\n",
    "\n",
    "Start by fetching `riva-api` with Helm, and examining the assets downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch riva-api with Helm\n",
    "!helm fetch https://helm.ngc.nvidia.com/nvidia/riva/charts/riva-api-2.8.1.tgz \\\n",
    "    --username='$oauthtoken' --password=$NGC_API_KEY --untar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97836b67",
   "metadata": {},
   "source": [
    "The configuration file, `values.yaml` contains a number of settings for the service including image details, credentials, and service type.  It also contains a list of ASR, NLP, and TTS models that will be downloaded and optimized upon initialization under `ngcModelConfigs:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807794b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat riva-api/values.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01feb9dc",
   "metadata": {},
   "source": [
    "The Helm Chart starts two containers:\n",
    "* `riva-model-init` - Responsible for fetching all of the model assets configured in `values.yaml` and their optimization for the target platform (appropriate TensorRT optimization will be executed).  After initialization is complete, this container will self-terminate.\n",
    "* `riva-speech-api` - Hosts Riva services after initialization is complete. \n",
    "\n",
    "Before proceeding, we'll need to make some edits to set the configurations in `values.yaml` to match our environment and limit the models deployed.  If we deploy all the possible models, we may run out of memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where Riva models are located in our class environment\n",
    "RIVA_MODEL_REPO = \"/dli_workspace/riva-full-model-repo\"\n",
    "!ls -al $RIVA_MODEL_REPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af8310",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 11.1.1 Exercise: Configure Helm Values and Deploy\n",
    "Modify the YAML file for our environment and deploy `riva-api` with Helm.  For our environment, the host path location for Riva `models`, `rmir`, and `artifacts` is `/dli_workspace/riva-full-model-repo`.  We also need to comment out all of the models listed to avoid unnecessary deployments as we already have the models we need in the `/dli_workspace/riva-full-model-repo` directory.\n",
    "\n",
    "Exercise:\n",
    "* Open the [values.yaml](riva-api/values.yaml) config file\n",
    "* Comment out all uncommented models under `ngcModelConfigs:`\n",
    "* Modify the `modelDeployVolume.hostPath.path` to reflect our environment\n",
    "* Modify `artifactDeployVolume.hostPath.path` to reflect our environment\n",
    "* Save the file\n",
    "* Check your work against the [solution](solutions/ex11.1.1.yaml) before moving on\n",
    "* Deploy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028902fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify values.yaml so that this cell verifies changes are correct\n",
    "# Check your work - your file should have the same uncommented models (none!) and folder paths as the solution!\n",
    "print(\"YOUR SETTINGS\\n=============\")\n",
    "!cat riva-api/values.yaml | grep -v \"^\\s*[#;]\" | sed -n '/ngcModelConfigs:/,/modelDeployVolume:/p' | sed ';$d'\n",
    "!cat riva-api/values.yaml | grep -v \"^\\s*[#;]\" | grep -A 20 modelDeployVolume: | grep 'DeployVolume\\|path'\n",
    "print(\"\\nSOLUTION SETTINGS\\n=================\")\n",
    "!cat solutions/ex11.1.1.yaml | grep -v \"^\\s*[#;]\" | sed -n '/ngcModelConfigs:/,/modelDeployVolume:/p' | sed ';$d'\n",
    "!cat solutions/ex11.1.1.yaml | grep -v \"^\\s*[#;]\" | grep -A 20 modelDeployVolume: | grep 'DeployVolume\\|path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddc177",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick Fix!\n",
    "!cp solutions/ex11.1.1.yaml riva-api/values.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f400d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env model_key_string=tlt_encode\n",
    "\n",
    "!helm install riva-api riva-api \\\n",
    "    --set ngcCredentials.password=`echo -n $NGC_API_KEY | base64 -w0` \\\n",
    "    --set ngcCredentials.email=$NGC_EMAIL \\\n",
    "    --set modelRepoGenerator.modelDeployKey=`echo -n model_key_string | base64 -w0`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7267a7b",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.2 Riva Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b42ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl describe pods riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f7823",
   "metadata": {},
   "source": [
    "At first, the models are downloading (this is reflected in the status), so we have to wait. Wait a minute and look at the status again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pods riva-api | grep -A 2 'Containers:\\|State:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pods riva-api | grep riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377b258",
   "metadata": {},
   "source": [
    "We need to wait until the status of the `riva-model-init` container changes from \"Waiting\" to \"Running\". You can keep executing the previous command to check as many times as needed.  Once `riva-model-init` is \"Running\", we should be able to view the Docker container logs. We need the name of the pod to view the logs, which we'll grab with a Linux `grep` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e00569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Grab the name\n",
    "RIVA_API_LONGNAME=$(kubectl describe pods riva-api | grep \"Name:         riva-api-\" | awk '{print $2}')\n",
    "echo \"The pod name is $RIVA_API_LONGNAME\"\n",
    "# Check the logs\n",
    "kubectl logs $RIVA_API_LONGNAME --container=riva-model-init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d538f",
   "metadata": {},
   "source": [
    "The logs should say that the models are already deployed and optimized and that the initialization has finished.  For example, they should consist of lines like:\n",
    "\n",
    "```\n",
    "2023-01-18 23:14:15,563 [INFO] Writing Riva model repository to '/data/models'...\n",
    "2023-01-18 23:14:15,563 [INFO] The riva model repo target directory is /data/models\n",
    "2023-01-18 23:14:23,844 [INFO] Using obey-precision pass with fp16 TRT\n",
    "2023-01-18 23:14:23,844 [WARNING] /data/models/riva-trt-riva_ner-nn-bert-base-uncased already exists, skipping deployment. \n",
    "```\n",
    "    \n",
    "Troubleshooting note:<br>\n",
    "If there is a mistake in the path configuration, then the initialization container will attempt to download all of the assets. The models take approximately 6GB of space and their target-specific optimization is a non-trivial task.  Therefore, this step can take 45+ minutes. If the logs are saying that Riva is downloading models, you can uninstall this helm deployment by executing `!helm uninstall riva-api`, correct the [values.yaml](riva-api/values.yaml) file, and try deploying again. \n",
    "\n",
    "When Riva model initialization is complete, Riva services will initialize. This can also take a while as all models need to be loaded to memory and verified, and there are quite a few models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f18cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -l $RIVA_MODEL_REPO/models\n",
    "!du -sh $RIVA_MODEL_REPO/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9154b08",
   "metadata": {},
   "source": [
    "Check to see if the service container, `riva-speech-api` is running yet.<br>\n",
    "Once it is, take a look at the logs for the container.  The logs should list all the models loaded and confirm that \"Riva Conversational AI Server listening on 0.0.0.0:50051\" in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat execution of this cell until riva-speech-api \"State\" is \"Running\" and \"Ready\" is \"True\"\n",
    "!kubectl describe pods riva-api | grep -A 2 'Containers:\\|State:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c54209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Grab the name\n",
    "RIVA_API_LONGNAME=$(kubectl describe pods riva-api | grep \"Name:         riva-api-\" | awk '{print $2}')\n",
    "\n",
    "echo \"The pod name is $RIVA_API_LONGNAME\"\n",
    "# Check the logs\n",
    "kubectl logs $RIVA_API_LONGNAME --container=riva-speech-api | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7362d39",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.3 Riva TTS Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86109a1c",
   "metadata": {},
   "source": [
    "If you have observed \"Riva Conversational AI Server listening on 0.0.0.0:50051\" in the logs, we are ready to run an application. We will query the API with a TTS example. More information on the API can be found [in the documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/tts-python-basics-and-customization-with-ssml.html)<br>\n",
    "\n",
    "First, import the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616df235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import librosa\n",
    "from time import time\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import riva.client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a4d89",
   "metadata": {},
   "source": [
    "Configure the connection to our server. As you might recall, the service is listening on port 50051. Lets try configuring localhost:50051.  Call the app and output an audio file to listen to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d69891",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff63a6",
   "metadata": {},
   "source": [
    "Next, we'll create a little function that sets the channel and submits a line of text to the `SynthesizeSpeech` model and returns an audio sample.  Then run the audio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_hz = 44100\n",
    "\n",
    "def test_tts(auth, input_text):\n",
    "    riva_tts = riva.client.SpeechSynthesisService(auth)    \n",
    "    req = { \n",
    "            \"language_code\"  : \"en-US\",\n",
    "            \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,   # Currently only LINEAR_PCM is supported\n",
    "            \"sample_rate_hz\" : sample_rate_hz,                          # Generate 44.1KHz audio\n",
    "            \"voice_name\"     : \"English-US.Female-1\"                    # The name of the voice to generate\n",
    "        }\n",
    "    req[\"text\"] = input_text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    return audio_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(test_tts(auth, \"Is it recognize speech or wreck a nice beach?\"), rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe445dc",
   "metadata": {},
   "source": [
    "Well, that didn't work... Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af55e25",
   "metadata": {},
   "source": [
    "## 11.3.1 Exercise: Pod IP with Port 50051\n",
    "\n",
    "When running Riva from within Kubernetes, our \"localhost\" IP (127.0.0.1) is not connected to the Riva services.  There are a few different pathways we could use to send our request.  The first is to select the Riva API pod IP address and send our requests there. The IP is listed in the pod description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod -o wide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get service --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf64f5",
   "metadata": {},
   "source": [
    "Replace the `POD_IP` with the actual IP value in the next cell and try it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961979f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO replace the POD_IP\n",
    "auth = riva.client.Auth(uri='POD_IP:50051')\n",
    "ipd.Audio(test_tts(auth, \"Is it recognize speech or wreck a nice beach?\"), rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189b4a2",
   "metadata": {},
   "source": [
    "Did that work?  There is another way as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490a550",
   "metadata": {},
   "source": [
    "## 11.3.2 Exercise: LoadBalancer IP with Port 50051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70559e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34623420",
   "metadata": {},
   "source": [
    "Alternatively, we could use the load balancer IP that is set up with a 50051 port mapping for requests.  \n",
    "\n",
    "Replace the `LOADBALANCER_IP` with the actual value in the next cell and try it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO replace the LOADBALANCER_IP\n",
    "auth = riva.client.Auth(uri='LOADBALANCER_IP:50051')\n",
    "ipd.Audio(test_tts(auth, \"Is it recognize speech or wreck a nice beach?\"), rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f1077",
   "metadata": {},
   "source": [
    "## 11.3.3 Exercise: Localhost with Mapped Port\n",
    "Connect to the external facing port mapped from the load balancer to localhost. In this case, this port is assigned randomly, so lets check what it is by looking at the port mapped to 50051 in the services list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f43b75",
   "metadata": {},
   "source": [
    "Replace the `MAPPED_PORT` with the actual value in the next cell and try it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab471bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO replace the MAPPED_PORT\n",
    "auth = riva.client.Auth(uri='localhost:MAPPED_PORT')\n",
    "ipd.Audio(test_tts(auth, \"Is it recognize speech or wreck a nice beach?\"), rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848a7ee",
   "metadata": {},
   "source": [
    "## 11.3.4 Upgrade the Service with Helm\n",
    "Load balancing is used to distribute tasks over a set of compute resources.  Since we have just one GPU and pod in our example, we do not need the load balancer.  We can turn it off by changing the service type in the `values.yaml` file executing the upgrade command. Here's what we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./riva-api/values.yaml | grep -A 4 service:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca04582",
   "metadata": {},
   "source": [
    "The [`helm upgrade` command](https://helm.sh/docs/helm/helm_upgrade/) has the form:\n",
    "\n",
    "```\n",
    "helm upgrade [RELEASE] [CHART] [flags]\n",
    "```\n",
    "\n",
    "   * CHART is the archive location of the `chart.yaml` file, `riva-api`\n",
    "   * RELEASE is be the specific name of the riva-api service deployed. \n",
    "   \n",
    "RELEASE is listed in the services names, so we can grab it from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b95d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Show the RELEASE value\n",
    "RELEASE=$(kubectl get svc -A | grep \"riva-api\"| awk '{print $2}')\n",
    "echo $RELEASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c5e68",
   "metadata": {},
   "source": [
    "### 11.3.4.1 Exercise: Upgrade the Service Type to NodePort\n",
    "Modify the YAML file for to change the service type from `LoadBalancer` to `NodePort` and upgrade it with Helm.\n",
    "\n",
    "Exercise:\n",
    "* Open the [values.yaml](riva-api/values.yaml) config file\n",
    "* Modify the \"service.type\" to \"NodePort\"\n",
    "* Save the file\n",
    "* Check your work against the [solution](solutions/ex11.3.4.1.yaml) before moving on\n",
    "* Upgrade the service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb491bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify values.yaml so that this cell verifies changes are correct\n",
    "# Check your work - your file should have the values as the solution!\n",
    "print(\"YOUR SETTING\")\n",
    "!cat ./riva-api/values.yaml | grep -A 4 service:\n",
    "print(\"\\nSOLUTION SETTING\")\n",
    "!cat solutions/ex11.3.4.1.yaml | grep -A 4 service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "RELEASE=$(kubectl get svc -A | grep \"riva-api\"| awk '{print $2}')\n",
    "helm upgrade $RELEASE riva-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44162aa3",
   "metadata": {},
   "source": [
    "### 11.3.4.2 Verify the Upgrade\n",
    "Since we have configured port 32222 as our NodePort, we should see the change now in our live service list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b710df",
   "metadata": {},
   "source": [
    "As a consequence, we have a known IP:PORT value to reliably expose the Riva server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979086d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:32222')\n",
    "ipd.Audio(test_tts(auth, \"Is it recognize speech or wreck a nice beach?\"), rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089172a",
   "metadata": {},
   "source": [
    "What did the code actually do? It executed a request to a TTS service transcribing the sentence provided, then generated an audio file with the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d73a01",
   "metadata": {},
   "source": [
    "---\n",
    "# 11.4 Riva NLP Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b8c1c",
   "metadata": {},
   "source": [
    "In the TTS example, we used the `SpeechSynthesisService` class to synthesize speech.  We can similarly make a requests with `NLPService` class and we'll try a couple of the NLP examples. Since there are several API tasks available for NLP, lets get a list by inspecting the class.  You can also review the code directly at https://github.com/nvidia-riva/python-clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "# List the callable objects.\n",
    "[method_name for method_name in dir(riva.client.NLPService)\n",
    "                  if callable(getattr(riva.client.NLPService, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d00ed5",
   "metadata": {},
   "source": [
    "## 11.4.1 `analyze_intent` API\n",
    "The `analyze_intent` API can be used to query an \"intent slot\" classifier. If we don't have a specific domain, this API can be leveraged with an additional text classification model to classify the domain of the input query before routing the text to the appropriate intent slot model.\n",
    "\n",
    "We'll keep things simple and use an example where the domain is known. This example skips execution of the domain classifier\n",
    "and proceeds directly to the intent slot model for the requested domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a875563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to return intent of a string\n",
    "def test_intent(auth, input_text):\n",
    "    riva_nlp = riva.client.NLPService(auth)\n",
    "    response = riva_nlp.analyze_intent(\n",
    "        input_string = input_text,\n",
    "        options = riva.client.AnalyzeIntentOptions(lang = 'en-US'))\n",
    "    return response\n",
    "\n",
    "auth = riva.client.Auth(uri='localhost:32222')\n",
    "print(test_intent(auth, \"How is the humidity today in San Francisco?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce860e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weather Intent queries\n",
    "queries = [\n",
    "    \"Is it currently cloudy in Tokyo?\",\n",
    "    \"What is the annual rainfall in Pune?\",\n",
    "    \"What is the humidity going to be tomorrow?\"\n",
    "]\n",
    "\n",
    "auth = riva.client.Auth(uri='localhost:32222')\n",
    "for q in queries:\n",
    "    print(q, '\\n', test_intent(auth, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f701323",
   "metadata": {},
   "source": [
    "## 11.4.2 `punctuate_text` API\n",
    "We can use this API to run the punctuation and capitalization model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to return punctuation for a list of strings\n",
    "plain_text_strings = [\n",
    "    \"add punctuation to this sentence\",\n",
    "    \"do you have any red nvidia shirts\",\n",
    "    \"i need one cpu four gpus and lots of memory \",\n",
    "    \"for my new computer it's going to be very cool\"\n",
    "]\n",
    "\n",
    "def test_punctuation(auth, input_texts):\n",
    "    riva_nlp = riva.client.NLPService(auth)\n",
    "    response = riva_nlp.punctuate_text(\n",
    "        input_strings = input_texts,\n",
    "        language_code = 'en-US')\n",
    "    return response.text\n",
    "\n",
    "punctuated = test_punctuation(auth, plain_text_strings)\n",
    "print(punctuated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5968d4",
   "metadata": {},
   "source": [
    "## 11.4.3 Shutdown\n",
    "Clean up your environment by shutting down Riva and K8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e366c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down K8s\n",
    "!minikube delete\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee0518",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Deployed Riva on K8s\n",
    "- Queried the TTS API, `SynthesizeSpeechRequest`\n",
    "- Learned how to access the Riva server from various IP:Port combinations\n",
    "- Queried the `AnalyzeIntent` and `TextTransform` NLP APIs\n",
    "\n",
    "Now that you've finished the hands-on portion of the course, you can work on the assessments to test your understanding and obtain a certificate!  Move on to the assessment questions in the course dashboard or the [coding assessment notebook](assessment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113a92b",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
