{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b7ae0e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e4ceb",
   "metadata": {},
   "source": [
    "# 8.0 ASR-NLP-TTS Deployment\n",
    "## (part of Lab 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b28fc7",
   "metadata": {},
   "source": [
    "In this notebook, you'll deploy a full pipeline with [NVIDIA Riva](https://developer.nvidia.com/riva). After building a \"plain vanilla\" end-to-end application using out-of-the-box models, you'll customize the pipeline for a specific restaurant use case.\n",
    "\n",
    "**[8.1 Full Pipeline With OOTB Models](#8.1-Full-Pipeline-With-OOTB-Models)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.1.1 Model Deployment](#8.1.1-Model-Deployment)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.1.2 Excercise: Riva Configuration](#8.1.2-Excercise:-Riva-Configuration)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.1.3 Riva Start Services](#8.1.3-Riva-Start-Services)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.1.4 OOTB Pipeline Demo](#8.1.4-OOTB-Pipeline-Demo)<br>\n",
    "**[8.2 ASR Customization](#8.2-ASR-Customization)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.2.1 Word Boosting](#8.2.1-Word-Boosting)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.2.2 Exercise: Negative Word Boost](#8.2.2-Exercise:-Negative-Word-Boost)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.2.3 Lexicon Customization](#8.2.3-Lexicon-Customization)<br>\n",
    "**[8.3 NER Customization](#8.3-NER-Customization)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.3.1 IOB Tagging](#8.3.1-IOB-Tagging)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.3.2 Restaurant Context for NER](#8.3.2-Restaurant-Context-for-NER)<br>\n",
    "**[8.4 TTS Customization](#8.4-TTS-Customization)<br>**\n",
    "**[8.5 Customized Restaurant Pipeline Demo](#8.5-Customized-Restaurant-Pipeline-Demo)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.5.1 Exercise: Run a Custom Pipeline](#8.5.1-Exercise:-Run-a-Custom-Pipeline)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[8.5.2 Shut Down Riva](#8.5.2-Shut-Down-Riva)<br>\n",
    "**[8.6 Shut Down the Kernel](#8.6-Shut-Down-the-Kernel)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9698bee",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you have:\n",
    "\n",
    "1. **NGC Credentials**<br>Be sure you have added your NGC credential as described in the [NGC Setup notebook](003_Intro_NGC_Setup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bb5c6",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.1 Full Pipeline With OOTB Models\n",
    "\n",
    "In previous notebooks, we took a close look at ASR and TTS speech models.  In this notebook, we'll add NLP (Natural Language Processing) models to build a full pipeline. NLP models interpret text in various ways so that action can be taken based on the text's meaning. We'll use the standard OOTB (out-of-the-box) models that Riva pulls from NGC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c52b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8.1.1 Model Deployment\n",
    "The default ASR and TTS service models were deployed earlier in their own model repositories, `/dli_workspace/riva-asr-model-repo` and `/dli_workspace/riva-tts-model-repo`.  For the full pipeline, we'll need to deploy default models for ASR, TTS, and NLP services.  The `riva_init.sh` command loads and builds models specific to the GPU you are using, but to save time for this course, these have been preloaded.\n",
    "\n",
    "The optimized NLP models are already located in the `/dli_workspace/riva-full-model-repo`, but lets go ahead and copy the ASR and TTS models there as well to save some build time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df010bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "WORKSPACE='/dli_workspace'\n",
    "RIVA_QS = WORKSPACE + \"/riva_quickstart\"\n",
    "RIVA_MODEL_REPO = WORKSPACE + \"/riva-full-model-repo\"\n",
    "!mkdir -p $RIVA_MODEL_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d99e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Copy all the ASR and TTS models for convenience (faster deployment)\n",
    "# Time is about 1-2 minutes for the copy\n",
    "cp -rn  /dli_workspace/riva-asr-model-repo/* \\\n",
    "    /dli_workspace/riva-full-model-repo/\n",
    "cp -rn  /dli_workspace/riva-tts-model-repo/* \\\n",
    "    /dli_workspace/riva-full-model-repo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062e20f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check to see what models are there now\n",
    "!ls $RIVA_MODEL_REPO/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d05f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8.1.2 Excercise: Riva Configuration\n",
    "\n",
    "Open [config.sh](dli_workspace/riva_quickstart/config.sh) and modify it to deploy all three services (ASR, NLP, TTS) using the `/dli_workspace/riva-full-model-repo` location that we've preloaded with all the models.  Save your work.\n",
    "\n",
    "If you're not sure what to change, take a peek at the [solution](solutions/ex8.1.2_config.sh)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222c057",
   "metadata": {},
   "source": [
    "Check your work.  The `diff` comparison in the following cell should have no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "!diff solutions/ex8.1.2_config.sh dli_workspace/riva_quickstart/config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ab4d0-e284-4e91-a193-5b0cddf7916c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick fix!\n",
    "!cp solutions/ex8.1.2_config.sh dli_workspace/riva_quickstart/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2c63d",
   "metadata": {},
   "source": [
    "## 8.1.3 Riva Start Services\n",
    "\n",
    "The `riva_init.sh` script downloads the Riva containers needed, downloads models listed in `config.sh`, and optimizes  models as required with [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt). Since we've already downloaded the containers and preloaded the optimized models, `riva_init.sh` won't have much to do, but it is provided here for completeness.\n",
    "\n",
    "The `riva_start.sh` script starts the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b0a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Riva \n",
    "!cd $RIVA_QS && bash riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Riva server (about 1 minute)\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8bf15",
   "metadata": {},
   "source": [
    "## 8.1.4 OOTB Pipeline Demo\n",
    "Now that we have the Riva server running with all the OOTB models, let's build an application that will:\n",
    "\n",
    "1. Transcribe audio into text (ASR)\n",
    "2. Find a name in the text (NER)\n",
    "3. Determine what text to output in response (DM)\n",
    "4. Output audio of the text response (TTS)\n",
    "\n",
    "<img src=\"images/pipeline/full_pipeline_ootb.png\">\n",
    "\n",
    "We've already explored how ASR (step 1) and TTS (step 4) work, but now we add a Named Entity Recognition (NER) model and a simple Dialog Manager (DM) to the pipeline.  \n",
    "\n",
    "NER is a Natural Language Processing (NLP) task.  NER, also referred to as entity chunking, identification, or extraction, is the task of detecting and classifying key information (entities) in text. In other words, an NER model takes a piece of text as input and for each word in the text, the model identifies a category the word belongs to. For example, in a sentence: \"Mary lives in Santa Clara and works at NVIDIA\", the NER model should detect that Mary is a person, Santa Clara is a location and NVIDIA is a company.\n",
    "\n",
    "The DM is responsible for keeping track of the conversation state and determining responses.  For our purposes, we will use very simple slot-filling to create a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89ee03",
   "metadata": {},
   "source": [
    "Begin by importing the Riva client and some other useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44893b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import io\n",
    "import time\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb2dc2",
   "metadata": {},
   "source": [
    "**Step 1: ASR**\n",
    "\n",
    "Create an ASR function to transcribe audio to text. Then give it a try using a simple sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ec845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function to transcribe text from audio\n",
    "def asr_predict(SAMPLE):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    riva_asr = riva.client.ASRService(auth)\n",
    "    # This example uses a .wav file with LINEAR_PCM encoding.\n",
    "    # read in an audio file from local disk\n",
    "    # Set up an offline/batch recognition request\n",
    "    config = riva.client.RecognitionConfig()\n",
    "    config.language_code = \"en-US\"                    # Language code of the audio clip\n",
    "    config.max_alternatives = 1                       # How many top-N hypotheses to return\n",
    "    config.enable_automatic_punctuation = True        # Add punctuation when end of VAD detected\n",
    "    config.audio_channel_count = 1     \n",
    "    with io.open(SAMPLE, 'rb') as fh:\n",
    "        content = fh.read()\n",
    "    response = riva_asr.offline_recognize(content, config)\n",
    "    transcript=response.results[0].alternatives[0].transcript\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE=\"/dli_workspace/data/audio_sample_resampled2.wav\"\n",
    "\n",
    "with io.open(SAMPLE, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "ipd.Audio(SAMPLE, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f79518",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript=asr_predict(SAMPLE)\n",
    "print(\"ASR Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e330973",
   "metadata": {},
   "source": [
    "**Step 2: NER**\n",
    "\n",
    "Create an NER function to find special words, such as persons, locations, and organizations, within the transcribed text.  For the sentence \"Hi, my name is Dana and I work for NVIDIA\", the NER model should recognize \"Dana\" as a name and \"NVIDIA\" as an organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function to extract entities from text\n",
    "def ner_predict(text):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    service = riva.client.NLPService(auth)\n",
    "    tokens, slots, slot_confidences, starts, ends = riva.client.extract_most_probable_token_classification_predictions(\n",
    "        service.classify_tokens(input_strings=text, model_name='riva_ner'))\n",
    "    return tokens[0],slots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens,slots=ner_predict(transcript)\n",
    "print(tokens)\n",
    "print(slots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1507ca6",
   "metadata": {},
   "source": [
    "**Step 3: DM**\n",
    "\n",
    "For demonstration purposes, we'll build a very basic dialog manager that just looks for a person's name in the sentence and uses that name in the response text (if it exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2412c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialog Manager\n",
    "def dm_predict(slots, tokens):\n",
    "    if \"PER\" in slots:\n",
    "        index = slots.index(\"PER\")\n",
    "        response=\"Hi \" + tokens[index] + \", how can I help you?\"\n",
    "    else:\n",
    "        response=\"Hi, how can I help you?\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = dm_predict(slots, tokens)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d67af",
   "metadata": {},
   "source": [
    "**Step 4: TTS**\n",
    "\n",
    "Finally, we'll use the TTS model to output the response sentence as audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cdac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_hz = 44100\n",
    "\n",
    "# helper function for more readable output\n",
    "def remove_braces(braced_text):\n",
    "    return braced_text.replace(\"{@\",\"\").replace(\"}\",\"\")\n",
    "\n",
    "# Define a Python function to create speech from text\n",
    "def tts_predict(text):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    riva_tts = riva.client.SpeechSynthesisService(auth)\n",
    "    req = { \n",
    "            \"language_code\"  : \"en-US\",\n",
    "            \"encoding\"       : riva.client.AudioEncoding.LINEAR_PCM ,   # Currently only LINEAR_PCM is supported\n",
    "            \"sample_rate_hz\" : sample_rate_hz,                          # Generate 44.1KHz audio\n",
    "            \"voice_name\"     : \"English-US.Female-1\"                    # The name of the voice to generate\n",
    "    }\n",
    "    req[\"text\"] = text\n",
    "    resp = riva_tts.synthesize(**req)\n",
    "    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "    return audio_samples, remove_braces(resp.meta.processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_samples, processed_text =tts_predict(response)\n",
    "print(processed_text)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ab520",
   "metadata": {},
   "source": [
    "We have all the pieces, and have tested them individually.  Now let's run it as one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together\n",
    "\n",
    "SAMPLE=\"/dli_workspace/data/audio_sample_resampled2.wav\"\n",
    "print(\"First Audio sample:\")\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "# get input audio duration\n",
    "d=librosa.get_duration(filename=SAMPLE)\n",
    "# call Riva ASR  \n",
    "transcript = asr_predict(SAMPLE)\n",
    "# call Riva NER\n",
    "tokens,slots = ner_predict(transcript)\n",
    "# call Dialog Manager\n",
    "dm_response = dm_predict(slots, tokens)\n",
    "# call Riva TTS\n",
    "synth_audio, processed_text = tts_predict(dm_response)\n",
    "\n",
    "time.sleep(d)\n",
    "print(\"Virtual Assistant Response:\")\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de09f0",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.2 ASR Customization\n",
    "\n",
    "Imagine an application with the same basic pipeline that receives restaurant orders.  It may be the case that the standard ASR model does not recognize a particular dish available on the restaurant menu, so recognition of that dish needs to be added to the dictionary.  For our example, let's say that \"couscous\" has been ordered.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbe744",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE=\"/dli_workspace/data/couscous-left.wav\"\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "# get inut audio duration\n",
    "d=librosa.get_duration(filename=SAMPLE)\n",
    "# call Riva ASR  \n",
    "transcript=asr_predict(SAMPLE)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47c4df",
   "metadata": {},
   "source": [
    "The spoken word, \"couscous\", is not correctly transcribed by the ASR.  Instead, it thinks the spelling is \"Css\".  Is that because it doesn't know the word, or because it just didn't recognize it?  We can check by searching for the correct spelling in the Conformer-CTC model lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeeddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CONFORMER_OFFLINE = \"conformer-en-US-asr-offline-ctc-decoder-cpu-streaming-offline\"\n",
    "LEXICON = os.path.join(RIVA_MODEL_REPO, \"models\", CONFORMER_OFFLINE, \"1\", \"lexicon.txt\")\n",
    "\n",
    "! grep couscous $LEXICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a83da9",
   "metadata": {},
   "source": [
    "The word \"couscous\" is already part of the Riva ASR vocabulary, but was not recognized as the likely transcription. Since this is a restaurant context, we want to improve the likelihood that \"couscous\" is transcribed, which we can do with word boosting. \n",
    "\n",
    "_Note: If we have a large list of words of interest like this, we could fine-tune the language model, which would make word boosting unnecessary._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb861e",
   "metadata": {},
   "source": [
    "## 8.2.1 Word Boosting \n",
    "\n",
    "Word boosting is the easiest customization of Riva ASR. The boosting  happens at the client side, when querying for transcription. The user can specify a list of words of interest that are most likely to appear and giving them new (higher) scores. The user-specified scores are used for decoding the output of the acoustic model.\n",
    "\n",
    "In order to boost the word \"couscous\", we can use the config [`riva.client.add_word_boosting_to_config()`](https://github.com/nvidia-riva/python-clients/blob/928c63273176a939500e01ce176c463f1606a1ff/riva_api/asr.py#L78) function to specify the list of words and their scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48395dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with word boosting\n",
    "def asr_predict_WB(SAMPLE, boost, score):\n",
    "    auth = riva.client.Auth(uri='localhost:50051')\n",
    "    riva_asr = riva.client.ASRService(auth)\n",
    "    # This example uses a .wav file with LINEAR_PCM encoding.\n",
    "    # read in an audio file from local disk\n",
    "    # Set up an offline/batch recognition request\n",
    "    config = riva.client.RecognitionConfig()\n",
    "    config.language_code = \"en-US\"                    # Language code of the audio clip\n",
    "    config.max_alternatives = 1                       # How many top-N hypotheses to return\n",
    "    config.enable_automatic_punctuation = True        # Add punctuation when end of VAD detected\n",
    "    config.audio_channel_count = 1     \n",
    "    riva.client.add_word_boosting_to_config(config, [boost], score)    # ****** WORD BOOSTING ******\n",
    "    with io.open(SAMPLE, 'rb') as fh:\n",
    "        content = fh.read()\n",
    "    response = riva_asr.offline_recognize(content, config)\n",
    "    transcript=response.results[0].alternatives[0].transcript\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7469ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE=\"/dli_workspace/data/couscous-left.wav\"\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "# transcibe while boosting couscous with the score 4.0\n",
    "transcript=asr_predict_WB(SAMPLE,\"couscous\", 4.0)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb79950",
   "metadata": {},
   "source": [
    "## 8.2.2 Exercise: Negative Word Boost\n",
    "\n",
    "It's also possible to boost with negative numbers to reduce the likelihood of a particular transcription.  Try a few boost numbers, both negative and positive, to see what value is required to get the correct transcription of \"couscous\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcibe while boosting couscous with various `myscore` values such as:  -4.0, 1.0, 2.0, 3.0\n",
    "myscore = -4.0\n",
    "\n",
    "SAMPLE=\"/dli_workspace/data/couscous-left.wav\"\n",
    "transcript=asr_predict_WB(SAMPLE,\"couscous\", myscore)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8cff5",
   "metadata": {},
   "source": [
    "## 8.2.3 Lexicon Customization\n",
    "\n",
    "The lexicon is a raw text file that contains the mapping of each word on the vocabulary to its tokenized format (separated by a tab). Tokens must be part of the acoustic model's vocabulary .  <br> For example:\n",
    "\n",
    "``` \n",
    "as      ▁as\n",
    "with    ▁with\n",
    "not     ▁not\n",
    "don't   ▁don ' t\n",
    "```\n",
    "\n",
    "Customizing the lexicon file creates explicit pronunciations of terms, in the form of tokenized sequences. \n",
    "Let's see this in action with the word `bruschetta` that could be pronounced `brusketa`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample audio\n",
    "\n",
    "SAMPLE=\"/dli_workspace/data/bruschetta_resampled.wav\"\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "# call Riva ASR  \n",
    "transcript=asr_predict(SAMPLE)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97986b",
   "metadata": {},
   "source": [
    "The transcription is \"Bruce Keta\", which is not accurate. Let's check if \"bruschetta\" is part of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0318149",
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep bruschetta $LEXICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac06302",
   "metadata": {},
   "source": [
    "The word \"bruschetta\" is in the lexicon, but it was not recognized by Riva ASR pipeline. The only pronunciation configured is `▁b ru s ch e t t a`. \n",
    "\n",
    "Let's provide more sequences to recognize the word when pronouncing it \"bruce keta\".  \n",
    "\n",
    "We'll need to stop the Riva server, customize the pronunciation, and then restart with the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ada75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Riva server. \n",
    "!bash $RIVA_QS/riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485afbed",
   "metadata": {},
   "source": [
    "Let's locate and load the tokenizer and test the sequence of tokens for the sentence \"Hi, my name is Dana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sentencepiece as spm\n",
    "\n",
    "# locate the _tokenizer.model in Riva models repo\n",
    "mydir = os.path.join(RIVA_MODEL_REPO, \"models\", CONFORMER_OFFLINE, \"1\")\n",
    "os.chdir(mydir)\n",
    "for file in glob.glob(\"*.model\"):\n",
    "    filename = file\n",
    "    \n",
    "tokenizer = os.path.join(RIVA_MODEL_REPO, \"models\", CONFORMER_OFFLINE, \"1\", filename)\n",
    "\n",
    "# Load the tokenizer\n",
    "s = spm.SentencePieceProcessor(model_file=tokenizer)\n",
    "\n",
    "# tokenize the sentence\n",
    "s.encode(\"Hi, my name is Dana\",out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d0278",
   "metadata": {},
   "source": [
    "To generate new lexicon possibilities for \"bruschetta\" using the `encode()` function. We need:\n",
    "- PRONUNCIATION: What the word or phrase should sound like (our example: \"bruce keta\")\n",
    "- TOKEN: The desired written form of the word (our example: \"bruschetta\")\n",
    "\n",
    "Let's query for five variants of the sequence of tokens leading to this sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd894d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"bruschetta\"\n",
    "PRONUNCIATION=\"bruce keta\"\n",
    "\n",
    "for n in range(5):\n",
    "    print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fa5dc",
   "metadata": {},
   "source": [
    "Add those pronunciation options to the Riva offline lexicon file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo -e \"bruschetta\\t▁b ru ce ▁k e t a\" >> $LEXICON\n",
    "!echo -e \"bruschetta\\t▁b ru c e ▁ k e t a\" >> $LEXICON\n",
    "!echo -e \"bruschetta\\t▁b r u c e ▁k e t a\" >> $LEXICON\n",
    "!echo -e \"bruschetta\\t▁ b ru c e ▁k e t a\" >> $LEXICON\n",
    "!echo -e \"bruschetta\\t▁ b ru ce ▁ k e t a\" >> $LEXICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Riva lexicon is updated\n",
    "! grep bruschetta $LEXICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e60d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the Riva server (about 1 minute)\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9ee75",
   "metadata": {},
   "source": [
    "Let's query the customized Riva ASR service again with the updated lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = \"/dli_workspace/data/bruschetta_resampled.wav\"\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "# call Riva ASR  \n",
    "transcript = asr_predict(SAMPLE)\n",
    "print(\"Riva transcription After Lexicon Mapping:\\n\", transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bcd19",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.3 NER Customization\n",
    "In our restaurant application, we will need to pick entities out of a conversation beyond names and organizations.  For example, we might like to identify cuisine or dishes in the conversation.  The [MIT Restaurant Corpus](https://groups.csail.mit.edu/sls/downloads/) dataset has labels identified with _IOB Tagging_, which is what we need for fine-tuning an NER model with NeMo.  \n",
    "\n",
    "The actual training for the NER model is out of scope for this course, but the process is covered in detail in the DLI course, [Building Transformer-Based Natural Language Processing Applications](https://www.nvidia.com/en-us/training/instructor-led-workshops/natural-language-processing/).  For this class, we will use a custom NER restaurant model that was fine-tuned in NeMo using the Restaurant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334dfeff",
   "metadata": {},
   "source": [
    "## 8.3.1 IOB Tagging\n",
    "\n",
    "The sentences and labels in the NER restaurant dataset map to each other with inside, outside, beginning (IOB) tagging. Anything separated by white space is a word, including punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0095ee4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's take a look at the data \n",
    "print('Text:')\n",
    "! head -n 8 /dli_workspace/data/restaurant/text_train.txt\n",
    "\n",
    "print('\\nLabels:')\n",
    "! head -n 8 /dli_workspace/data/restaurant/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78489ea5",
   "metadata": {},
   "source": [
    "The first eight lines of the training dataset are mapped to eight lines of labels. For example, look at the 6th line, \"a place that serves soft serve ice cream\":\n",
    "\n",
    "```text\n",
    "a    place    that    serves    soft    serve    ice    cream\n",
    "O    O        O       O         B-Dish  I-Dish   I-Dish I-Dish\n",
    "```\n",
    "\n",
    "The IOB tags indicate that \"soft\" is the beginning of a \"Dish\" entity, and that the next three words, \"serve ice cream\" are also part of that entity.  The full entity identified is therefore \"soft serve ice cream\" as a \"Dish\". None of the other words in the sentence are identified as entities.  \n",
    "\n",
    "The possible entity labels for this dataset are listed in [label_ids.csv](dli_workspace/data/restaurant/label_ids.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2bd04",
   "metadata": {},
   "source": [
    "## 8.3.2 Restaurant Context for NER\n",
    "Let's try the basic OOTB NER model with a few restaurant queries.  In this example, we'll use NeMo rather than the Riva client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8da298",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "pretrained_ner_model = nemo_nlp.models.TokenClassificationModel.from_pretrained(model_name=\"ner_en_bert\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b17952",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a list of queries for inference\n",
    "request_bruschetta = \"I would like to order a bruschetta for 6pm\"\n",
    "request_pasta = \"Can you recommend a good Italian pasta?\"\n",
    "\n",
    "queries = [request_bruschetta, request_pasta]\n",
    "results = pretrained_ner_model.add_predictions(queries)\n",
    "\n",
    "for query, result in zip(queries, results):\n",
    "    print()\n",
    "    print(f'Query : {query}')\n",
    "    print(f'Result: {result.strip()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a26dbf",
   "metadata": {},
   "source": [
    "If our NER thinks \"bruschetta\" is an organization, it will be difficult to design a DM that detects what dish a customer wants to order! \n",
    "Let's try this again with a customized model based on the Restaurant dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbba2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_ner_model = nemo_nlp.models.TokenClassificationModel.restore_from(restore_path=\"/dli_workspace/riva-full-model-repo/ner_restaurant.nemo\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c65c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = my_ner_model.add_predictions(queries)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ed766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new definition of ner_predict using the custom model\n",
    "def ner_predict(text):\n",
    "    results = my_ner_model.add_predictions([text])\n",
    "    return results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9cac5",
   "metadata": {},
   "source": [
    "## 8.3.2 Restaurant DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Options ={\"bruschetta\": [\"Tomato and Olive Oil\", \"Mozarella and Basil\", \"Cherry Tomato and Garlic\"], \"pasta\": [\"Spaghetti Marinara\", \"Fettuccine Alfredo\", \"Linguini and Clams\"]}\n",
    "\n",
    "# Create a response with options for specific dishes\n",
    "def dm_predict_restaurant(res):\n",
    "    if \"B-Dish\" in res:\n",
    "        index = res.index(\"B-Dish\")\n",
    "        dish = res[:index-1].split().pop()\n",
    "        if dish in Options.keys():\n",
    "            list_options = Options[dish]\n",
    "            response = \"What \" + dish + \" option would you like? We have : \"\n",
    "            for l in list_options:\n",
    "                 response += l + \". \"\n",
    "        else:\n",
    "            response = dish + \". What else would you like?\"\n",
    "    else:\n",
    "        response = \"Hi, how can I help you?\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_response_bruschetta = dm_predict_restaurant(ner_predict(request_bruschetta))\n",
    "print(\"\\n{}\".format(request_bruschetta))\n",
    "print(dm_response_bruschetta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_response_pasta = dm_predict_restaurant(ner_predict(request_pasta))\n",
    "print(\"\\n{}\".format(request_pasta))\n",
    "print(dm_response_pasta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4777b",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.4 TTS Customization\n",
    "The correct pronunciation for \"bruschetta\" may be up for debate depending on where in the world you are.  For our application, we need to settle on a consistent pronunciation, so assume we want that \"bruce keta\" type of pronunciation we focused on earlier. Without any customization, the TTS model does not pronounce it that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate speech with the OOTB TTS model\n",
    "dm_response = dm_response_bruschetta\n",
    "synth_audio, processed_text =tts_predict(dm_response)\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8924ea",
   "metadata": {},
   "source": [
    "We can try different pronunciations with the NeMo aligner model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612fd8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import AlignerModel\n",
    "aligner = AlignerModel.from_pretrained(\"tts_en_radtts_aligner_ipa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"broo sketah\"\n",
    "text_g2p = aligner.tokenizer.g2p(input_string)\n",
    "print(text_g2p)\n",
    "text_tokens = aligner.tokenizer(input_string)\n",
    "print(text_tokens)\n",
    "print(\"\\n\" + ''.join(text_g2p))\n",
    "synth_audio, processed_text =tts_predict(''.join(text_g2p))\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"brous KETA\"\n",
    "text_g2p = aligner.tokenizer.g2p(input_string)\n",
    "print(text_g2p)\n",
    "text_tokens = aligner.tokenizer(input_string)\n",
    "print(text_tokens)\n",
    "print(\"\\n\" + ''.join(text_g2p))\n",
    "synth_audio, processed_text =tts_predict(''.join(text_g2p))\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dabd83",
   "metadata": {},
   "source": [
    "After trying a few, alter the input to the TTS model with a simple `replace()` in the string output from the DM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031882eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate speech \n",
    "custom_dm_response = dm_response.replace(\"bruschetta\", \"BROO SKETAH\") \n",
    "\n",
    "synth_audio, processed_text =tts_predict(custom_dm_response)\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e78c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 8.5 Customized Restaurant Pipeline Demo\n",
    "\n",
    "We've customized several parts of our pipeline:\n",
    "1. ASR - added detection of additional pronunciations with the lexicon\n",
    "2. NER - incorporated a fine-tuned NeMo model for restaurant context\n",
    "3. DM - added refinements for our simple example\n",
    "4. TTS - added pronunciation substitution\n",
    "\n",
    "<img src=\"images/pipeline/restaurant_pipeline.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b8797",
   "metadata": {},
   "source": [
    "## 8.5.1 Exercise: Run a Custom Pipeline\n",
    "\n",
    "In the following cell, complete the \"TODO\" sections to run the full pipeline with customizations:\n",
    "1. ASR customized lexicon for \"bruschetta\" (this should already be in place)\n",
    "2. NER customized NeMo model\n",
    "3. DM with the restaurant options\n",
    "4. TTS with substituted pronunciation for \"bruschetta\"\n",
    "\n",
    "If you get stuck, you can check the [solution](solutions/ex8.5.1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6339ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurant Pipeline\n",
    "# Put it all together\n",
    "\n",
    "SAMPLE=\"/dli_workspace/data/bruschetta_resampled.wav\"\n",
    "print(\"First Audio sample:\")\n",
    "ipd.display(ipd.Audio(SAMPLE, rate=sample_rate_hz, autoplay=True))\n",
    "\n",
    "# get input audio duration\n",
    "d=librosa.get_duration(filename=SAMPLE)\n",
    "\n",
    "# TODO call Riva ASR  \n",
    "# TODO call NeMo NER\n",
    "# TODO call Dialog Manager\n",
    "# TODO call Riva TTS\n",
    "\n",
    "time.sleep(d)\n",
    "print(\"Virtual Assistant Response:\")\n",
    "ipd.display(ipd.Audio(synth_audio, rate=sample_rate_hz, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca8149",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8.5.2 Shut Down Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Riva server \n",
    "!bash $RIVA_QS/riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bafad-2df9-47cf-b641-f99985cf82b4",
   "metadata": {},
   "source": [
    "---\n",
    "# 8.6 Shut Down the Kernel\n",
    "<h3 style=\"color:red;\">Important!</h3>\n",
    "\n",
    "From the menu above, choose ***Kernel->Shut Down Kernel*** to fully clear GPU memory before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d8eb4",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Built a full conversational AI pipeline\n",
    "- Customized ASR, NLP, and TTS in a full pipeline\n",
    "- Built a full restaurant context customized pipeline\n",
    "\n",
    "This concludes the TTS portion of the course.<br>\n",
    "Next, you'll work with deployment of Riva at scale using Kubernetes, starting with [Enabling GPU within Kubernetes](009_K8s_Enable.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8da155",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
