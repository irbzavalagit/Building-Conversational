{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff386d2",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ff76e",
   "metadata": {},
   "source": [
    "# 9.0 Enabling GPU within a Kubernetes (K8s) Cluster\n",
    "## (part of Lab 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac66b5",
   "metadata": {},
   "source": [
    "<img src=\"images/k8s/kubernetes_stack_0.png\" style=\"float: right;\">\n",
    "In this notebook, you'll learn how to prepare a Kubernetes cluster for GPU acceleration full production deployment of conversational AI applications.<br><br>\n",
    "\n",
    "**[9.1 Launch a K8s Cluster](#9.1-Launch-a-K8s-Cluster)<br>**\n",
    "**[9.2 Deploy a CUDA Test Application](#9.2-Deploy-a-CUDA-Test-Application)<br>**\n",
    "**[9.3 Add GPU Awareness to K8s](#9.3-Add-GPU-Awareness-to-K8s)<br>**\n",
    "**[9.4 Interact with GPU Resources in K8s](#9.4-Interact-with-GPU-Resources-in-K8s)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[9.4.1 Exercise: Configure Pod](#9.4.1-Exercise:-Configure-Pod)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[9.4.2 Final Checks and Shutdown](#9.4.2-Final-Checks-and-Shutdown)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[9.4.2.1 Exercise: Delete a Pod](#9.4.2.1-Exercise:-Delete-a-Pod)<br>\n",
    "\n",
    "In the previous parts of the class, you deployed NVIDIA Riva using basic shell commands. As convenient as this method is during development, it becomes impractical when deploying to production, that is, when managing larger numbers of servers and services. \n",
    "\n",
    "[Kubernetes](https://kubernetes.io/), also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. \n",
    "In this part of the class, we will first launch a K8s cluster, enable the cluster for GPU acceleration and interact with those resources. This is our first step toward monitoring, managing, and deploying conversational AI applications in production. Monitoring and deployment will be covered in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e938dc",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you are starting with a clean environment.  Ensure that by stopping any previous Kubernetes installation and all docker containers, then looking at our environment's state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7061d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check running docker containers. This should be empty.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8af4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If not empty,\n",
    "# Clear Docker containers to start fresh...\n",
    "!docker kill $(docker ps -q)\n",
    "\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fc5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deletes local Kubernetes cluster if it exists\n",
    "!minikube delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3e00d",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.1 Launch a K8s Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688d884",
   "metadata": {},
   "source": [
    "A [Kubernetes cluster](https://kubernetes.io/docs/concepts/overview/components/) consists of a set of worker machines (physical or virtual), called nodes, that run containerized applications. Every cluster has at least one worker node, though it can also support thousands of nodes! For this class, we will use [Minikube](https://minikube.sigs.k8s.io/docs/), which allows us to deploy a local and self-contained Kubernetes cluster with a single node. \n",
    "\n",
    "Review the class hardware resources available and launch the K8s cluster.\n",
    "\n",
    "We can see details and status of the available GPU using the `nvidia-smi` command.\n",
    "\n",
    "<img src=\"images/k8s/nvidia_smi.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39794855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What GPU are we using and how much memory does it have?\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea1871",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What type of CPU processor(s) are we using?\n",
    "!cat /proc/cpuinfo | grep \"model name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea6814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many processors are available?\n",
    "!nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7ba33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch the K8s cluster using Minikube\n",
    "!minikube start --driver=none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97deda3",
   "metadata": {},
   "source": [
    "Once the cluster is successfully launched, we expect to see a number of containers running.  Check this by executing `docker ps` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eeaf4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Listing the Kuberenetes components deployed\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb0e1b",
   "metadata": {},
   "source": [
    "We should now have access to the [kubectl command line tool](https://kubernetes.io/docs/reference/kubectl/overview/), which is used to interact with the cluster. List the nodes and services in the cluster using the `kubectl get` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a695a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List nodes in the cluster\n",
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa19ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all services deployed\n",
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93400d38",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.2 Deploy a CUDA Test Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e6d51",
   "metadata": {},
   "source": [
    "Next, we will deploy a simple GPU-accelerated application. This is a toy application which randomly generates two very large vectors and adds them. Print out the YAML configuration file needed to deploy the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beecc3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the configuration directory\n",
    "CONFIG_DIR='/dli/task/kubernetes-config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6e41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review the application we will deploy\n",
    "!cat $CONFIG_DIR/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac0bf4",
   "metadata": {},
   "source": [
    "The main difference between a YAML file specifying a GPU-accelerated application compared to one specifying a non-GPU-accelerated application, is the configuration of the GPU resources required. In our case, we have created a basic configuration requesting a single NVIDIA GPU by setting `resources: limits: nvidia.com/gpu:` to 1. \n",
    "\n",
    "To deploy an application, execute the `kubectl apply` command, specifying the YAML configuration file with the `-f` file option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95967dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the application\n",
    "!kubectl apply -f $CONFIG_DIR/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f808f91",
   "metadata": {},
   "source": [
    "Once deployed, we can observe the status of a pod created with `kubectl get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde467c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76183d79",
   "metadata": {},
   "source": [
    "At this stage, the application is in the \"Pending\" state. <br>\n",
    "Why do you think this is case? Do you think its just the fact we have not given the application enough time to launch? Or do you think there are other reasons for this behavior? Try executing the same command again to see if the status changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d59c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking again. Is it still pending?\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db584784",
   "metadata": {},
   "source": [
    "So the application is indeed in the \"Pending\" state and it will remain like that irrespective of the amount of time we wait. Why? Begin to answer this by looking at the configuration of the available nodes (in our case we just have one). In particular, look for any NVIDIA-specific configuration using the `kubectl describe` command, as this will help us identify GPU resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca86b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can we see the GPU?\n",
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3b5ed",
   "metadata": {},
   "source": [
    "Can you find anything? Try again, filtering the output with `grep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce95571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look for the lines containing the word \"nvidia\"\n",
    "!kubectl describe nodes | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f17dd6",
   "metadata": {},
   "source": [
    "We did not find anything. That would explain why the application is still pending. Our cluster is not aware of the presence of the GPU.  The cluster is unable to schedule the execution since our YAML required GPU resources, but they are for all intents and purposes unavailable. We need to add the NVIDIA GPU device plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a09da",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.3 Add GPU Awareness to K8s\n",
    "To take advantage of GPU acceleration on Kubernetes, install the [NVIDIA GPU plugin](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#deploying-nvidia-gpu-device-plugin) to the cluster. Before adding it, look at the status without the plugin  with `kubectl get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d1b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to find the GPU device plugin. Not there \n",
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d7385",
   "metadata": {},
   "source": [
    "To install the NVIDIA GPU plugin, we can use the Kubernetes package manager [Helm](https://helm.sh/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718a440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the device plugin with the Helm package manager\n",
    "!helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\\n",
    "   && helm repo update\n",
    "!helm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n",
    "  --namespace nvidia-device-plugin \\\n",
    "  --create-namespace \\\n",
    "  --version 0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce43ec9",
   "metadata": {},
   "source": [
    "Check the status again to make sure the plugin was deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c57655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now the device plugin \"nvidia-device-plugin-*\" should be \"Running\" after a \"ContainerCreating\" status\n",
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0459b",
   "metadata": {},
   "source": [
    "We should now see the NVIDIA-specific configuration listed against the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ae387",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we should see Allocable GPUs\n",
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149b719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look for the lines containing the word nvidia\n",
    "!kubectl describe nodes | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821e1ea",
   "metadata": {},
   "source": [
    "As we deployed the GPU device plugin, what do you think happened to our application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c6e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's check the application again\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f78c3",
   "metadata": {},
   "source": [
    "Our application executed successfully when the GPU resources became available. In fact, it has now completed so we can have a look at its execution logs with `kubectl logs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af98e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b40917",
   "metadata": {},
   "source": [
    "Check the list of Helm charts installed with the `helm list` command (see the [Helm documentation](https://helm.sh/docs/helm/helm_list/)). The `--filter` option allows filtering by name.  Use the `--output` option to specify the output format (\"json\", \"table\", or \"yaml\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7b260",
   "metadata": {},
   "source": [
    "Now, let's delete the Kubernetes pod `gpu-operator-test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa54d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's delete the pod\n",
    "!kubectl delete pod gpu-operator-test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e356641",
   "metadata": {},
   "source": [
    "Congratulations! You deployed a GPU accelerated applicaiton with Kuberenetes. So far, we have specified that we want a single GPU without specifying which GPU we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccfe3d",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.4 Interact with GPU Resources in K8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd957491",
   "metadata": {},
   "source": [
    "Now, let's see how to get more control over the GPU-accelerated cluster. Being able to control the GPU type, or the MIG ([Multi-Instance GPU](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)) partition on an Ampere GPU is very important as GPUs vary in terms of computational capability, memory, and cost. The MIG allows users to fragment the GPU into as many as 7 (on A100) partitions. This allows more granular control over the resources in the cluster and better application isolation. \n",
    "\n",
    "In order to control the GPU type, we'll add the `gpu-feature-discovery` plugin and deploy it with Helm. This plugin can be configured with several options, as described in the [gpu-feature-discovery repository](https://github.com/NVIDIA/gpu-feature-discovery#deployment-via-helm). One of the most interesting options when working with Ampere GPUs is the ability to support MIG partitions. The feature discovery plugin can be deployed with the following configurable features:\n",
    "\n",
    "\n",
    "|Feature|Description|Default|\n",
    "|-|-|-|\n",
    "|`failOnInitError`|Fail if there is an error during initialization of any label sources|\"true\"|\n",
    "|`sleepInterval`|Time to sleep between labeling|\"60s\"|\n",
    "|`migStrategy`|Pass the desired strategy for labeling MIG devices on GPUs that support it [none | single | mixed]|\"none\"|\n",
    "|`nfd.deploy`|When set to true, deploy NFD as a subchart with all of the proper parameters set for it|\"true\"|\n",
    "\n",
    "In this class, we are not using Ampere GPUs, so we will do a simple install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160d703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install feature discovery with the Helm package manager\n",
    "!helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery \\\n",
    "    && helm repo update\n",
    "!helm upgrade -i nvgfd nvgfd/gpu-feature-discovery \\\n",
    "  --version 0.7.0 \\\n",
    "  --namespace gpu-feature-discovery \\\n",
    "  --create-namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a0bbc",
   "metadata": {},
   "source": [
    "Let's look at additional information that we have about our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f8330",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Looking for all of the NVIDIA related information\n",
    "!kubectl describe nodes | grep \"nvidia.com\" -A 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ed633",
   "metadata": {},
   "source": [
    "You should see a wide range of GPU-specific information, including the driver and CUDA information, as well as which GPU is in use from `nvidia.com/gpu.product`.\n",
    "\n",
    "This is probably an NVIDIA A10, unless you are running the class on an alternative GPU. Recall that we deployed our test application `gpu-operator-test` with a generic \"GPU\".  It is possible to deploy it with more specific information regarding the GPU. \n",
    "\n",
    "A new YAML file, `gpu-pod-A10.yaml`, is already prepared. Let's inspect it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5cb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review the application we are deploying\n",
    "!cat $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d931ac",
   "metadata": {},
   "source": [
    "As you might have noticed, the YAML was configured to deploy on an A100 GPU, which is not available in the class. Go ahead and deploy the application anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c9c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8747bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl get pods gpu-operator-test-a100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff274f7",
   "metadata": {},
   "source": [
    "Just as we saw in the earlier non-GPU case, the deployment is in the \"Pending\" state and it will remain in this state until an A100 GPU becomes available or it is terminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f0337",
   "metadata": {},
   "source": [
    "## 9.4.1 Exercise: Configure Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06e9c0",
   "metadata": {},
   "source": [
    "Modify the YAML file and deploy the `gpu-operator-test` application on the correct GPU.\n",
    "Open the [gpu-pod-A10.yaml](kubernetes-config/gpu-pod-A10.yaml) config file and make those changes:\n",
    "* Change the pod name to \"gpu-operator-test-a10\"\n",
    "* Set the GPU product to the GPU name you found earlier (such as \"NVIDIA-A10G\") instead of the A100\n",
    "\n",
    "Check your work against the [solution](solutions/ex9.4.1.yaml) before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e374560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO modify gpu-pod-A10.yaml so that this cell verifies changes are correct\n",
    "# Check your work - you'll get no output if the files match\n",
    "!diff $CONFIG_DIR/gpu-pod-A10.yaml solutions/ex9.4.1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f74e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick Fix!\n",
    "!cp solutions/ex9.4.1.yaml $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfaef3",
   "metadata": {},
   "source": [
    "Next, deploy the `gpu-operator-test-a10` pod using the modified [gpu-pod-A10.yaml](kubernetes-config/gpu-pod-A10.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1c5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f71250",
   "metadata": {},
   "source": [
    "## 9.4.2 Final Checks and Shutdown\n",
    "It might take several seconds, but the application should deploy and finish successfully.  Rerun the next cell until the status shows the test is \"completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f2f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test-a10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c639d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test-a10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb4306",
   "metadata": {},
   "source": [
    "### 9.4.2.1 Exercise: Delete a Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caaab0c",
   "metadata": {},
   "source": [
    "Delete the Kubernetes pod `gpu-operator-test-a10`. Check the [solution](solutions/ex9.4.2.ipynb) before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e5694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO delete the pod\n",
    "!kubectl #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64dd64",
   "metadata": {},
   "source": [
    "Before moving forward to the next notebook, shut down K8s and clean up the docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bd1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shut down K8s\n",
    "!minikube delete\n",
    "# Shut down running docker containers\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15389a1",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Launched a K8s cluster\n",
    "- Interacted with K8s using `kubectl`\n",
    "- Installed plugins with Helm\n",
    "- Enabled GPU acceleration and GPU feature discovery\n",
    "- Deployed an application\n",
    "\n",
    "Next, you'll monitor activity on the cluster. Move on to [Monitoring GPU within Kubernetes Cluster](010_K8s_Monitor.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba9f69",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
