{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6.0 TTS Pipeline with NVIDIA NeMo\n",
    "## (part of Lab 2)\n",
    "\n",
    "In this notebook, you'll run a text-to-speech (TTS) pipeline to generate audio samples using NeMo's pretrained models.  We'll zoom in a bit to examine the steps,  including normalization, spectrogram generation, and vocoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6.1 Text Normalization](#6.1-Text-Normalization)<br>**\n",
    "**[6.2 Grapheme to Phoneme Conversion (G2P)](#6.2-Grapheme-to-Phoneme-Conversion-(G2P))<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.2.1 Pronunciation Dictionary](#6.2.1-Pronunciation-Dictionary)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.2.2 Run G2P](#6.2.2-Run-G2P)<br>\n",
    "**[6.3 Spectrogram Generator](#6.3-Spectrogram-Generator)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.3.1 Load FastPitch](#6.3.1-Load-FastPitch)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.3.2 Run FastPitch](#6.3.2-Run-FastPitch)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.3.3 Customize the Pitch](#6.3.3-Customize-the-Pitch)<br>\n",
    "**[6.4 Vocoder](#6.4-Vocoder)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.4.1 Load HiFiGAN](#6.4.1-Load-HiFiGAN)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.4.2 Run HiFiGAN Inference](#6.4.2-Run-HiFiGAN-Inference)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.4.3 Customize Audio Duration and Pitch](#6.4.3-Customize-Audio-Duration-and-Pitch)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.4.4 Exercise: Happy Emotion](#6.4.4-Exercise:-Happy-Emotion)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.4.5 Customize Phonemes](#6.4.5-Customize-Phonemes)<br>\n",
    "**[6.5 Shut Down the Kernel](#6.5-Shut-Down-the-Kernel)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tts/tts_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Text Normalization\n",
    "\n",
    "\n",
    "As opposed to the inverse text normalization (ITN) process applied to the ASR pipeline, text normalization (TN) converts a written format to its verbalized form. This preprocessing step is necessary for the text-to-speech (TTS) pipeline.\n",
    "\n",
    "For example, the TN module converts the standard textual measures format, *the height is 84.6cm*, to its verbalized form ,*the height is eighty four point six centimeters*.\n",
    "\n",
    "The NeMo text normalizer uses two composite _weighted finite-state transducers_ (WFSTs):\n",
    "- Classifier: Tag input into semiotic tokens (e.g. currency, ordinal number, street address)\n",
    "- Verbalizer: Render a tagged token into conventional written form\n",
    "\n",
    "Learn more about NeMo TN in the documentation: [NeMo Text Normalization](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/v1.0.0/nemo_text_processing/text_normalization.html).\n",
    "\n",
    "\n",
    "In the next few cells, we load the TN for English and run the normalization on some text using the `nemo_text_processing.text_normalization.normalize.Normalizer.normalize` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "text_normalizer = Normalizer(input_case=\"cased\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Your total bill is $25.45\"\n",
    "normalized_text = text_normalizer.normalize(text)\n",
    "print(\"Input text     : \", text)\n",
    "print(\"Normalized text: \", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NVIDIA was founded in 04/05/1993, 29 years ago.\"\n",
    "normalized_text = text_normalizer.normalize(text)\n",
    "print(\"Input text     : \", text)\n",
    "print(\"Normalized text: \", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Mrs. Smith.\"\n",
    "normalized_text = text_normalizer.normalize(text)\n",
    "print(\"Input text     : \", text)\n",
    "print(\"Normalized text: \", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to output the details of tagged text provided by the TN model by setting the `verbose=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Your total bill is $25.45\"\n",
    "\n",
    "normalized_text = text_normalizer.normalize(text,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Grapheme to Phoneme Conversion (G2P)\n",
    "\n",
    "For text representation, TTS pipelines use phonemes (small units of the sound) as representations instead of graphemes (orthographic symbols). \n",
    "This is because phonemes are a richer representation for speech synthesis as they include pronunciation information. There are different phonetic representation standards supporting English (among others) such as [ARPAbet](https://en.wikipedia.org/wiki/ARPABET) and [International Phonetic Alphabet (IPA)](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet). \n",
    "\n",
    "For example: \n",
    "- Graphemes example: *\"you have a cute dog\"*\n",
    "- IPA phonemes example: *\"ˈju ˈhæv A ˈkjut ˈdɔɡ\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Pronunciation Dictionary\n",
    "\n",
    "Mapping between graphemes and phoneme for common words in a language can be specified where needed. A list of supported phonemes and IPA symbols can be found in the [Phoneme Support documentation] (https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-phones.html#phoneme-support).\n",
    "\n",
    "Examples:\n",
    "- NVIDIA   --> ɛnˈvɪdiə\n",
    "- DECEMBER --> dɪˈsɛmbɚ\n",
    "\n",
    "The dictionary can be extended with the new words and their corresponding desired pronunciation.\n",
    "\n",
    "An extended version of the English IPA dictionary is included in this course instance. You can also find the dictionary on [NeMo Repository](https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tts_dataset_files/ipa_cmudict-0.7b_nv22.10.txt).\n",
    "\n",
    "Let's first have a look the IPA dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the IPA dictionary \n",
    "IPA_DICTIONARY = \"/opt/NeMo/scripts/tts_dataset_files/ipa_cmudict-0.7b_nv22.10.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep 'JANUARY' $IPA_DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep 'NVIDIA' $IPA_DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Run G2P\n",
    "\n",
    "Grapheme to phoneme conversion (G2P) converts graphemes to phonemes. \n",
    "\n",
    "Let's instantiate a G2P converter with NeMo using the `IPAG2P` module, which converts sentences from graphemes to IPA phoneme representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.torch.g2ps import IPAG2P\n",
    "\n",
    "# instantiate G2P with our dictionary \n",
    "ipa = IPAG2P(phoneme_dict=IPA_DICTIONARY, use_stresses=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string1 = \"My name is Dana.\"\n",
    "\n",
    "# Convert G2P\n",
    "phonemes = ipa(input_string1)\n",
    "print('Phonemes: ', ''.join(phonemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In English, phonemes of a word have the same level of pronunciation intensity (or stress). IPA support two stress symbol:\n",
    "- `ˈ` primary stress symbol: the stronger phoneme of the word\n",
    "- `ˌ` secondary stress symbol: moderately strong phoneme of the word\n",
    "\n",
    "Use `use_stresses=True` to set G2P with stress symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate G2P with stress symbols\n",
    "ipa_stress = IPAG2P(phoneme_dict=IPA_DICTIONARY, use_stresses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert G2P with stress symbols\n",
    "phonemes_stress = ipa_stress(input_string1)\n",
    "print('Phonemes: ', ''.join(phonemes_stress))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try with another example including a proper name with Polish pronunciation: *Grzywaczewski*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string2 = \"my name is Adam Grzywaczewski\"\n",
    "\n",
    "# Convert G2P with stress symbols\n",
    "phonemes_stress = ipa_stress(input_string2)\n",
    "print('Phonemes: ', ''.join(phonemes_stress))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Grzywaczewski* is an out-of-vocabulary word (OOV), meaning it is not part of the IPA dictionary. \n",
    "\n",
    "We can customize G2P models with OOV words, like proper names such as *Grzywaczewski*, to provide their phonetic form [*ɡzɪvɑˈtʃɛvski*] to improve the quality of the synthesized text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Spectrogram Generator\n",
    "\n",
    "\n",
    "<img src=\"images/tts/TTS_spec.png\">\n",
    "In the NeMo library, TTS models are defined under the `nemo.collections.tts.models` method. \n",
    "\n",
    "To load a TTS model, we can restore the parameters from a local `.nemo` model. \n",
    "\n",
    "Alternatively, pretrained models can be loaded from NVIDIA repository on NGC using the `from_pretrained(...)` method that downloads and initializes model directly from the cloud. To check the list of available pretrained models, please use the `list_available_models()` method.\n",
    "\n",
    "NeMo support Tacotron2, GlowTTS, TalkNet, FastPitch, FastSpeech2, Mixer-TTS, and Mixer-TTS-X spectrogram generators. In this lab, we will explore the FastPitch spectrogram generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "#import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Reduce logging messages for this notebook\n",
    "#from nemo.utils import logging\n",
    "#logging.setLevel(logging.ERROR)\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.helpers.helpers import regulate_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display list of models\n",
    "def display_list_available_models(model):\n",
    "    print ( \"list of available models:\")\n",
    "    for m in model.list_available_models():\n",
    "        print (\"   \", \"\\033[1;34m\", m.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the list of FastPitch models available in NeMo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check list of fastpitch models available\n",
    "display_list_available_models(FastPitchModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 Load FastPitch\n",
    "\n",
    "FastPitch is a fully-parallel Transformer-based architecture with prosody control over pitch and phoneme duration. The model predicts pitch contours during inference that can be modified to add more expressiveness to the generated speech.\n",
    "\n",
    "In this lab, we will use the English FastPitch model [`tts_en_fastpitch_ipa`](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_fastpitch) that generates mel spectrograms from text input. This model is trained on the [LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/) sampled at 22050Hz, and has been tested on generating female English voices with an American accent.\n",
    "\n",
    "Let's load the model and examine its architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load spectrogram generator\n",
    "fastpitch = FastPitchModel.from_pretrained(\"tts_en_fastpitch_ipa\").eval().cuda()\n",
    "sample_rate = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot fastpitch architecture \n",
    "summary(fastpitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of parameters this FastPitch model has!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Run FastPitch\n",
    "\n",
    "Let's generate the mel spectrograms of an input text string with FastPitch. \n",
    "\n",
    "The FastPitch inference function takes as input the tokenized text. So let's first generate the tokenized version of a text string using `fastpitch.parse()` model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"My name is Dana and I work for NVIDIA.\"\n",
    "\n",
    "# Normalize the text and convert it into individual phonemes/tokens.\n",
    "tokens= fastpitch.parse(input_string, normalize=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the FastPitch inference to generate the mel spectrograms. In the following cell, we define the `text_to_spectrogram` function to run inference and display function for the generated mel spectrograms and the predicted pitch and duration used to generate the mel spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastPitch inference\n",
    "def text_to_spectrogram(inp, pace=1.0, durs=None, pitch=None):\n",
    "    with torch.no_grad():\n",
    "        tokens = fastpitch.parse(inp)\n",
    "        spec, _, durs_pred, _, pitch_pred, *_ = fastpitch(text=tokens, durs=durs, pitch=pitch, speaker=None, pace=pace)\n",
    "    return spec, durs_pred, pitch_pred\n",
    "\n",
    "# Plot mel spectrogram\n",
    "def plot_spectrogram(spec):\n",
    "    # Plot the spectrogram\n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "    fig.suptitle('Mel-Spectrogram', fontsize=20)\n",
    "    _ = plt.pcolormesh(spec[0].cpu().numpy(), cmap='viridis')\n",
    "\n",
    "# plot the pitch\n",
    "def plot_pitch(pitch, durs):\n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch_pred, _ = regulate_len(durs, pitch.unsqueeze(-1))\n",
    "    pitch_pred = pitch_pred.squeeze(-1)\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    #ax.title.set_text('Pitch', fontdict = {'font.size':22})\n",
    "    fig.suptitle('Pitch', fontsize=20)\n",
    "    ax.plot(pitch_pred.cpu().numpy()[0], color='blue', linewidth=1)\n",
    "\n",
    "# Plot aligned mel spectrogram and pitch\n",
    "def plot_spectrogram_pitch(spec, pitch, durs):\n",
    "    # FastPitch predicts one pitch value per token. To plot it, we have to expand the token length to the spectrogram length\n",
    "    pitch_pred, _ = regulate_len(durs, pitch.unsqueeze(-1))\n",
    "    pitch_pred = pitch_pred.squeeze(-1)\n",
    "    pitch_pred = pitch_pred + 6\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    fig.suptitle('Alligned Mel-Spectrogram and Pitch', fontsize=20)\n",
    "    ax.plot(pitch_pred.cpu().numpy()[0], color='blue', linewidth=1, alpha=0.5)\n",
    "    _ = plt.pcolormesh(spec[0].cpu().numpy(), cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run FastPitch\n",
    "spec_pred, durs_pred, pitch_pred = text_to_spectrogram(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mel spectrogram  \n",
    "plot_spectrogram(spec_pred)\n",
    "\n",
    "# Plot the pitch\n",
    "plot_pitch(pitch_pred, durs_pred)\n",
    "\n",
    "# Plot alligned mel spectrogram and pitch\n",
    "plot_spectrogram_pitch(spec_pred, pitch_pred, durs_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 Customize the Pitch\n",
    "To customize the pitch and phoneme's durations, we can operate directly on the pitch and duration vectors before generating the mel spectrogram. \n",
    "\n",
    "Let's flatten the pitch and visualize the effect of this operation on the generated mel spectrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the pitch\n",
    "pitch_flat = pitch_pred * 0\n",
    "# Now we can pass it to the model\n",
    "spec_flat, durs_flat_pred, pitch_flat_pred = text_to_spectrogram(input_string, pitch=pitch_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mel spectrogram  \n",
    "plot_spectrogram(spec_flat)\n",
    "\n",
    "# Plot the pitch\n",
    "plot_pitch(pitch_flat, durs_pred)\n",
    "\n",
    "# Plot alligned mel spectrogram and pitch\n",
    "plot_spectrogram_pitch(spec_flat, pitch_flat, durs_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Vocoder\n",
    "\n",
    "NeMo supports WaveGlow, SqueezeWave, UniGlow, MelGAN, HiFiGAN, UnivNet, Tacotron2, and RadTTS vocoders. In this lab, we will explore the HiFiGAN vocoder.\n",
    "\n",
    "Let's check the available HiFiGAN models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_list_available_models(HifiGanModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.1 Load HiFiGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HiFiGAN is a generative adversarial network (GAN) model that generates audio from mel spectrograms. The architecture is based on transposed convolutions to upsample mel spectrograms to audio.\n",
    "\n",
    "\n",
    "In this lab, we will use the HiFiGAN model [`tts_hifigan`](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_hifigan) that generate audio from Mel-spectrograms. This model is trained on [LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/) sampled at 22050Hz, and has been tested on generating female English voices with an American accent. \n",
    "\n",
    "Let's load the model and examine its architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the models from NGC\n",
    "hifigan = HifiGanModel.from_pretrained(\"tts_hifigan\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(hifigan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2 Run HiFiGAN Inference\n",
    "\n",
    "Let's now generate speech synthesis from the spectrogram generated by the FastPitch mel spectrogram generator.\n",
    "\n",
    "The `spectrogram_to_audio` function uses the NeMo `hifigan.convert_spectrogram_to_audio` module to generate the speech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_to_audio(spec):\n",
    "    with torch.no_grad():\n",
    "        audio = hifigan.convert_spectrogram_to_audio(spec=spec).to('cpu').numpy()\n",
    "    return  audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate speech synthesis steps: \n",
    "- Generate the mel spectrogram from the input text using `text_to_spectrogram`\n",
    "- Generate audio from the mel spectrogram using `spectrogram_to_audio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"My name is Dana and I work for NVIDIA\"\n",
    "\n",
    "# FastPitch mel spectrogram generator\n",
    "spec_pred, durs_pred, pitch_pred = text_to_spectrogram(input_string)  \n",
    "\n",
    "# HiFiGAN audio generator\n",
    "audio_pred=spectrogram_to_audio(spec_pred)\n",
    "    \n",
    "# Display generated speech sythesis\n",
    "ipd.display(ipd.Audio(audio_pred, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.3 Customize Audio Duration and Pitch\n",
    "\n",
    "We can customize the duration and pitch of the mel spectrogram to modify speech synthesis. \n",
    "\n",
    "We'll start with duration. The pace value of the speaker can be specified to the spectrogram generator. Higher values result in a faster speaking pace.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durations \n",
    "fast_pace=1.5\n",
    "slow_pace=0.8\n",
    "\n",
    "# Fast speech\n",
    "spec_pred_fast, durs_pred, pitch_pred = text_to_spectrogram(input_string, pace=fast_pace)  \n",
    "\n",
    "audio_fast=spectrogram_to_audio(spec_pred_fast)\n",
    "print(\"Fast Speech:\")\n",
    "ipd.display(ipd.Audio(audio_fast, rate=22050))\n",
    "    \n",
    "# Slow speech\n",
    "spec_pred_slow, durs_pred, pitch_pred = text_to_spectrogram(input_string, pace=slow_pace) \n",
    "\n",
    "audio_slow=spectrogram_to_audio(spec_pred_slow)\n",
    "print(\"Slow Speech:\")\n",
    "ipd.display(ipd.Audio(audio_slow, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the pitch by generating audio with higher, lower, and flat pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch cutomization\n",
    "pitch_higher=pitch_pred+1.0\n",
    "pitch_lower=pitch_pred-1.0\n",
    "pitch_flat = pitch_pred * 0\n",
    "\n",
    "# Higher speech\n",
    "spec_pred, durs_pred, pitch_pred = text_to_spectrogram(input_string,pitch=pitch_higher)  \n",
    "\n",
    "audio_pred=spectrogram_to_audio(spec_pred)\n",
    "print(\"Higher Pitch:\")\n",
    "ipd.display(ipd.Audio(audio_pred, rate=22050))\n",
    "\n",
    "# Lower speech\n",
    "spec_pred, durs_pred, pitch_pred = text_to_spectrogram(input_string,pitch=pitch_lower)  \n",
    "\n",
    "audio_pred=spectrogram_to_audio(spec_pred)\n",
    "print(\"Lower Pitch:\")\n",
    "ipd.display(ipd.Audio(audio_pred, rate=22050))\n",
    "\n",
    "# Flat speech\n",
    "spec_pred, durs_pred, pitch_pred = text_to_spectrogram(input_string,pitch=pitch_flat)  \n",
    "\n",
    "audio=spectrogram_to_audio(spec_pred)\n",
    "print(\"Flat Speech:\")\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another sentence and generate speech synthesis with a sadness emotion.\n",
    "- Generate a vanilla speech synthesis\n",
    "- Modify the pitch to add some sadness: We need to lower the pitch ($\\times 0.75$)and shift it to the left ($-0.75$)\n",
    "- Generate a new  speech synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string_2 = \"Oh, I'm very sorry for mispronouncing your name.\"\n",
    "\n",
    "# Pitch cutomization for sad emotion\n",
    "\n",
    "spec_pred, durs_pred, pitch_pred = text_to_spectrogram(input_string_2)\n",
    "audio_pred=spectrogram_to_audio(spec_pred)\n",
    "print(\"First Audio sample:\")\n",
    "ipd.display(ipd.Audio(audio_pred, rate=22050))\n",
    "\n",
    "# sad speech \n",
    "pitch_sad = (pitch_pred)*0.75-0.75\n",
    "spec_sad, durs_sad_pred, _ = text_to_spectrogram(input_string_2, pitch=pitch_sad)\n",
    "audio_sad=spectrogram_to_audio(spec_sad)\n",
    "print(\"Saddened Speech:\")\n",
    "ipd.display(ipd.Audio(audio_sad, rate=22050))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.4 Exercise: Happy Emotion\n",
    "\n",
    "Generate speech synthesis with happy emotion by customizing the pitch.\n",
    "- Generate a vanilla speech synthesis\n",
    "- Operate on the pitch to add some happiness. We need to make a higher pitch and shift it to the right.\n",
    "- Generate a new speech synthesis\n",
    "\n",
    "Change the <i><strong style=\"color:green;\">#FIXME</strong></i>s in the cell below. If you get stuck, refer to the [solution](solutions/ex6.4.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch cutomization for happy emotion\n",
    "\n",
    "input_string_3 = \"I'm very happy for you.\"\n",
    "\n",
    "# Generate a vanilla speech synthesis\n",
    "spec_pred, durs_pred, pitch_pred = #FIXME\n",
    "\n",
    "audio_pred=spectrogram_to_audio(spec_pred)\n",
    "print(\"First Audio sample:\")\n",
    "ipd.display(ipd.Audio(audio_pred, rate=22050))\n",
    "\n",
    "# Happy speech \n",
    "pitch_happy = #FIXME\n",
    "spec_happy, durs_happy_pred, pitch_happy_pred = #FIXME\n",
    "\n",
    "audio_happy=spectrogram_to_audio(spec_happy)\n",
    "print(\"Happy Speech:\")\n",
    "ipd.display(ipd.Audio(audio_happy, rate=22050))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.5 Customize Phonemes\n",
    "\n",
    "Let's try with another example including a proper name with Polish pronunciation: *Grzywaczewski*.\n",
    "Let's start by generating the speech synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"His name is Adam Grzywaczewski and he works for NVIDIA\"\n",
    "\n",
    "tokens = fastpitch.parse(input_string)\n",
    "spec, _, durs_pred, _, pitch_pred, *_ = fastpitch(text=tokens, durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "audio=spectrogram_to_audio(spec)\n",
    "print(\"First Audio sample:\")\n",
    "ipd.display(ipd.Audio(audio, rate=22050))\n",
    "print(\"Tokens: \",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad pronunciation!\n",
    "\n",
    "One IPA pronunciation of `Grzywaczewski` could be `ɡzɪvɑˈtʃɛvski`\n",
    "\n",
    "So let's tokenize the customized IPA representation to get the preferred list of tokens representing `ɡzɪvɑˈtʃɛvski`\n",
    "Then, simply replace in the original tokenized sentence the `Grzywaczewski` sequence of tokens by the new pronunciation tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load radTTS aligner to get tokenized \n",
    "from nemo.collections.tts.models import AlignerModel\n",
    "aligner = AlignerModel.from_pretrained(\"tts_en_radtts_aligner_ipa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the original customized IPA\n",
    "tokens_original = aligner.tokenizer.encode_from_g2p(list('GRZYWACZEWSKI'))\n",
    "tokens_preferred = aligner.tokenizer.encode_from_g2p(list('ɡzɪvɑˈtʃɛvski'))\n",
    "print(\"\\n=== Tokens===\")\n",
    "print(\"GRZYWACZEWSKI: \", tokens_original)\n",
    "print(\"ɡzɪvɑˈtʃɛvski: \", tokens_preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate tensor to replace values for tokens_original with tokens_preferred\n",
    "import torch\n",
    "custom_tokens=tokens\n",
    "print(\"Full original token string:\",custom_tokens)\n",
    "\n",
    "for counter in range(len(custom_tokens[0])): \n",
    "    sublist = custom_tokens[0][counter:counter+len(tokens_original)]\n",
    "    if sublist.tolist() == tokens_original:\n",
    "        print(\"Found it at {}\".format(counter))\n",
    "        tensor_before = custom_tokens[:,0:counter]\n",
    "        tensor_sublist = custom_tokens[:,counter:counter+len(tokens_original)]\n",
    "        tensor_after = custom_tokens[:,counter+len(tokens_original):]\n",
    "        break\n",
    "print(\"Before: {}\".format(tensor_before))\n",
    "print(\"Sublist: {}\".format(tensor_sublist))\n",
    "tensor_replacement = torch.tensor([tokens_preferred], device='cuda:0')\n",
    "print(\"Replacement: {}\".format(tensor_replacement))\n",
    "print(\"After: {}\".format(tensor_after))\n",
    "\n",
    "new_tokens = torch.cat((tensor_before, tensor_replacement, tensor_after), dim=1, out=None)\n",
    "print(\"Final token string: {}\".format(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec, _, durs_pred, _, pitch_pred, *_ = fastpitch(text=new_tokens, durs=None, pitch=None, speaker=None, pace=1.0)\n",
    "audio_sad=spectrogram_to_audio(spec)\n",
    "print(\"Customized Phonemes sample:\")\n",
    "ipd.display(ipd.Audio(audio_sad, rate=22050))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.5 Shut Down the Kernel\n",
    "<h3 style=\"color:red;\">Important!</h3>\n",
    "\n",
    "From the menu above, choose ***Kernel->Shut Down Kernel*** to fully clear GPU memory before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Normalized written text to speech-appropriate text\n",
    "- Converted graphemes to phonemes\n",
    "- Used the FastPitch model to create a spectrograph\n",
    "- Converted spectrograms to audio with the HiFiGAN vocoder model\n",
    "- Customized speech output\n",
    "\n",
    "Next, you'll deploy the model on Riva. Move on to [TTS Pipeline Deployment with Riva](007_TTS_Deployment.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9750c3195c8cb9412c65c8ba8b36c6ba2b82b23ddd61f39d29e01b403b049680"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
